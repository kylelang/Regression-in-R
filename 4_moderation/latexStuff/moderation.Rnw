%%% Title:    DSS Stats & Methods: Lecture 8
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2020-09-24

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{caption}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{Moderation, Interactions, and Polynomials}
\subtitle{Statistics \& Methodology Lecture 8}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{}

\begin{document}

<<setup, include=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(lattice)
library(xtable)
library(DAAG)
library(MLmetrics)

source("../../../code/supportFunctions.R")

dataDir <- "../data/"

options(width = 60)
opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@


\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}

  \begin{enumerate}
  \item Testing for moderation with MLR models
    \va
  \item Polynomial regression
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Moderation}

  So far we've been discussing \emph{additive models}.
  \vb
  \begin{itemize}
  \item Additive models allow us to examine the partial effects of several
    predictors on some outcome.
    \vc
    \begin{itemize}
    \item The effect of one predictor does not change based on the values of 
      other predictors.
    \end{itemize}
  \end{itemize}
  \va
  Now, we'll discuss \emph{moderation}.
  \vb
  \begin{itemize}
  \item Moderation allows us to ask \emph{when} one variable, $X$, affects
    another variable, $Y$.
    \vc
    \begin{itemize}
    \item We're considering the conditional effects of $X$ on $Y$ given certain 
      levels of a third variable $Z$.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Equations}

  In additive MLR, we might have the following equation:
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \varepsilon
  \end{align*}
  This additive equation assumes that $X$ and $Z$ are independent
  predictors of $Y$.\\
  \va
  When $X$ and $Z$ are independent predictors, the following are true:
  \vb
  \begin{itemize}
  \item $X$ and $Z$ \emph{can} be correlated.
    \vb
  \item $\beta_1$ and $\beta_2$ are \emph{partial} regression
    coefficients.
    \vb
  \item \red{The effect of $X$ on $Y$ is the same at \textbf{all levels} of
    $Z$, and the effect of $Z$ on $Y$ is the same at \textbf{all
      levels} of $X$.}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Additive Regression}
  
  The effect of $X$ on $Y$ is the same at \textbf{all levels} of $Z$.
  
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{figures/3d_data_plot}
    \end{column}
    
    \begin{column}{0.1\textwidth}
      \begin{center}\Huge{$\rightarrow$}\end{center}
    \end{column}
    
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{figures/response_surface_plot0}
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Moderated Regression}
  
  The effect of $X$ on $Y$ varies \textbf{as a function} of $Z$.
  
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{figures/3d_data_plot}
    \end{column}
    
    \begin{column}{0.1\textwidth}
      \begin{center}\Huge{$\rightarrow$}\end{center}
    \end{column}
    
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{figures/response_surface_plot}
    \end{column}
  \end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Equations}
  
  The following derivation is adapted from \citet{hayes:2017}.
  \vb
  \begin{itemize}
  \item When testing moderation, we hypothesize that the effect of $X$ on $Y$ 
    varies as a function of $Z$.  
    \vb
  \item We can represent this concept with the following equation:
    \begin{align}
      Y = \beta_0 + f(Z)X + \beta_2Z + \varepsilon \label{fEq}
    \end{align}
    \vx{-8}
    \pause
  \item If we assume that $Z$ linearly (and deterministically) affects the 
    relationship between $X$ and $Y$, then we can take:
    \begin{align}
      f(Z) = \beta_1 + \beta_3Z \label{ssEq}
    \end{align}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Equations}
  
  \begin{itemize}
  \item Substituting Equation \ref{ssEq} into Equation \ref{fEq} leads to:
    \begin{align*}
      Y = \beta_0 + (\beta_1 + \beta_3Z)X + \beta_2Z + \varepsilon
    \end{align*}
    \pause
  \item Which, after distributing $X$ and reordering terms, becomes:
    \begin{align*}
      Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \varepsilon
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Testing Moderation}
  Now, we have an estimable regression model that quantifies the linear 
  moderation we hypothesized.
  \vb
  \begin{center}\ovalbox{$Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + 
      \varepsilon$}\end{center}
  \vc
  \begin{itemize}
  \item To test for significant moderation, we simply need to test the 
    significance of the interaction term, $XZ$.
    \begin{itemize}
    \item Check if $\hat{\beta}_3$ is significantly different from zero.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretation}
  
  Given the following equation:
  \begin{align*}
    Y = \hat{\beta}_0 + \hat{\beta}_1X + \hat{\beta}_2Z + \hat{\beta}_3XZ + 
    \hat{\varepsilon}
  \end{align*}
  \vx{-16}
  \begin{itemize}
  \item $\hat{\beta}_3$ quantifies the effect of $Z$ on the focal effect (the $X 
    \rightarrow Y$ effect).
    \vc
    \begin{itemize}
    \item For a unit change in $Z$, $\hat{\beta}_3$ is the expected change in
      the effect of $X$ on $Y$.
    \end{itemize}
    \vb
  \item $\hat{\beta}_1$ and $\hat{\beta}_2$ are \emph{conditional effects}.
    \vc
    \begin{itemize}
      \item Interpreted where the other predictor is zero.
        \vc
      \item For a unit change in $X$, $\hat{\beta}_1$ is the expected change in
        $Y$, when $Z = 0$.
        \vc
      \item For a unit change in $Z$, $\hat{\beta}_2$ is the expected change in
        $Y$, when $X = 0$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{Example}
  
  Still looking at the \emph{diabetes} dataset.
  \va 
  \begin{itemize}
  \item We suspect that patients' BMIs are predictive of their average blood 
    pressure. 
    \va 
  \item We further suspect that this effect may be differentially expressed 
    depending on the patients' LDL levels.
  \end{itemize}
  
\end{frame}

\watermarkoff%-----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<echo = FALSE>>=
dDat <- readRDS("../data/diabetes.rds")
@ 

<<>>=
## Focal Effect:
out0 <- lm(bp ~ bmi, data = dDat)
partSummary(out0, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Additive Model:
out1 <- lm(bp ~ bmi + ldl, data = dDat)
partSummary(out1, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Moderated Model:
out2 <- lm(bp ~ bmi * ldl, data = dDat)
partSummary(out2, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualizing the Interaction}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      We can get a better idea of the patterns of moderation by plotting the 
      focal effect at conditional values of the moderator.
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
m1 <- mean(dDat$ldl)
s1 <- sd(dDat$ldl)

dDat$ldlLo  <- dDat$ldl - (m1 - s1)
dDat$ldlMid <- dDat$ldl - m1
dDat$ldlHi  <- dDat$ldl - (m1 + s1)

outLo  <- lm(bp ~ bmi*ldlLo, data = dDat)
outMid <- lm(bp ~ bmi*ldlMid, data = dDat)
outHi  <- lm(bp ~ bmi*ldlHi, data = dDat)

b0Lo <- coef(outLo)[1]
b1Lo <- coef(outLo)["bmi"]

b0Mid <- coef(outMid)[1]
b1Mid <- coef(outMid)["bmi"]

b0Hi <- coef(outHi)[1]
b1Hi <- coef(outHi)["bmi"]

x    <- seq(min(dDat$bmi), max(dDat$bmi), 0.1)
dat1 <- data.frame(x    = x,
                   yLo  = b0Lo + b1Lo * x,
                   yMid = b0Mid + b1Mid * x,
                   yHi  = b0Hi + b1Hi * x)

p1 <- ggplot(data = dDat, aes(x = bmi, y = bp)) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16))
p2 <- p1 + geom_point(colour = "gray") +
    geom_line(mapping = aes(x = x, y = yLo, colour = "Mean LDL - 1 SD"),
              data    = dat1,
              size    = 1.5) +
    geom_line(mapping = aes(x = x, y = yMid, colour = "Mean LDL"),
              data    = dat1,
              size    = 1.5) +
    geom_line(mapping = aes(x = x, y = yHi, colour = "Mean LDL + 1 SD"),
              data    = dat1,
              size    = 1.5) +
    xlab("BMI") +
    ylab("BP")

p2 + scale_colour_manual(name = "", values = c("Mean LDL" = "black",
                                               "Mean LDL - 1 SD" = "red",
                                               "Mean LDL + 1 SD" = "blue")
                         ) +
    theme(legend.justification = c(1, 0), legend.position = c(0.975, 0.025))
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Probing the Interaction}

  A significant estimate of $\beta_3$ tells us that the effect of $X$ on $Y$ 
  depends on the level of $Z$, but nothing more.
  \vb
  \begin{itemize}
  \item The plot on the previous slide gives a descriptive illustration of the 
    pattern, but does not support statistical inference.
    \vc
    \begin{itemize}
    \item The three conditional effects we plotted look different, but we cannot 
      say much about how they differ with only the plot and $\hat{\beta}_3$.
    \end{itemize}
    \vb
  \item This is the purpose of \emph{probing} the interaction.
    \vc
    \begin{itemize}
    \item Try to isolate areas of $Z$'s distribution in which $X \rightarrow Y$
      effect is significant and areas where it is not.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Probing the Interaction}

  The most popular method of probing interactions is to do a so-called 
  \emph{simple slopes} analysis.
  \vc
  \begin{itemize}
  \item Pick-a-point approach
    \vc
  \item Spotlight analysis
  \end{itemize}
  \vb
  In simple slopes analysis, we test if the slopes of the conditional effects 
  plotted above are significantly different from zero.
  \vc
  \begin{itemize}
  \item To do so, we test the significance of \emph{simple slopes}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Simple Slopes}

  Recall the derivation of our moderated equation:
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \varepsilon
  \end{align*}
  We can reverse the process by factoring out $X$ and reordering terms:
  \begin{align*}
    Y = \beta_0 + (\beta_1 + \beta_3Z)X + \beta_2Z + \varepsilon
  \end{align*}
  Where $f(Z) = \beta_1 + \beta_3Z$ is the linear function that shows how the 
  relationship between $X$ and $Y$ changes as a function of $Z$.\\
  \vc
  \begin{center}
  \ovalbox{$f(Z)$ is the \emph{simple slope}.}
  \end{center}
  \begin{itemize}
  \item By plugging different values of $Z$ into $f(Z)$, we get the value of the 
    conditional effect of $X$ on $Y$ at the chosen level of $Z$.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Significance Testing of Simple Slopes}

  The values of $Z$ used to define the simple slopes are arbitrary.
  \vc
  \begin{itemize}
  \item The most common choice is: $\left\{ (\bar{Z} - SD_Z), \bar{Z},
    (\bar{Z} + SD_Z) \right\}$
    \vc
  \item You could also use interesting percentiles of $Z$'s distribution.
  \end{itemize}
  \vb
  The standard error of a simple slope is given by:
  \begin{align*}
    SE_{f(Z)} = \sqrt{SE_{\beta_1}^2 + 2Z \cdot \text{COV}(\beta_1, \beta_3) + 
      Z^2 SE_{\beta_3}^2}
  \end{align*}
  So, you can test the significance of a simple slope by constructing a Wald 
  statistic or confidence interval using $\hat{f}(Z)$ and $SE_{f(Z)}$:
  \begin{align*}
    t = \frac{\hat{f}(Z)}{SE_{f(Z)}},~~
    CI = \hat{f}(Z) \pm t_{crit} \times SE_{f(Z)}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interaction Probing}
  
  When probed the interaction with simple slopes analysis:
  \vb
  \begin{enumerate}
    \item Choose interesting values of the moderator, $Z$. \label{chooseZ}
      \vb
    \item Check the significance of the focal effect, $X \rightarrow Y$, at the 
      $Z$ values chosen in Step \ref{chooseZ}. \label{testSS}
      \vb
    \item Use the results from Step \ref{testSS} to get an idea of where in 
      $Z$'s distribution the focal effect is or is not significant.
  \end{enumerate}
  \va
  \pause
  We saw manual calculations for the the quantities needed, but there is a 
  simpler way:
  \vc
  \begin{itemize}
    \item \textsc{Centering}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Centering}
  
  Centering shifts the scale of a variable up or down by subtracting a constant 
  (e.g., the variable's mean) from each of its observations.
  \vc
  \begin{itemize}
  \item The most familiar form of center is \emph{mean centering}.
    \vc
  \item We can center on any value.
    \vc
    \begin{itemize}
    \item When probing interactions, we can center $Z$ on the interesting values
      we choose to define the simple slopes.
      \vc
    \item Due to the interpretation of conditional effects, running the model 
      with $Z$ centered on a specific value automatically provides a test of the 
      simple slope for that value of $Z$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Probing via Centering}
  
  Say we want to do a simple slopes analysis to test the conditional effect of 
  $X$ on $Y$ at three levels of $Z = \{Z_1, Z_2, Z_3\}$.
  \vb
  \begin{itemize}
  \item All we need to do is fit the following three models:
    \begin{align*}
      Y &= \beta_0 + \beta_1X + \beta_2(Z - Z_1) + \beta_3 X(Z - Z_1) + 
      \varepsilon\\[10pt]
      Y &= \beta_0 + \beta_1X + \beta_2(Z - Z_2) + \beta_3 X(Z - Z_2) + 
      \varepsilon\\[10pt]
      Y &= \beta_0 + \beta_1X + \beta_2(Z - Z_3) + \beta_3 X(Z - Z_3) + 
      \varepsilon
    \end{align*}
    \pause
    \vx{-12}
  \item The default output for $\hat{\beta}_1$ provides tests of the simple 
    slopes.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Create transformed predictors by centering on critical values of the 
  moderator, $Z_{LDL}$.
<<>>=
zMean     <- mean(dDat$ldl)
zSD       <- sd(dDat$ldl)
dDat$zCen <- dDat$ldl - zMean 
dDat$zHi  <- dDat$ldl - (zMean + zSD)
dDat$zLo  <- dDat$ldl - (zMean - zSD)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Test the simple slope of $X_{BMI} \rightarrow Y_{BP}$ at 1 $SD$ below the mean 
  of $Z_{LDL}$.
<<>>=
out2.1 <- lm(bp ~ bmi*zLo, data = dDat)
partSummary(out2.1, -c(1, 2))
@
The estimated slope for \texttt{bmi}, $\hat{\beta}_1 = 
\Sexpr{round(coef(out2.1)["bmi"], 3)}$, is the simple slope.
 
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Test the simple slope of $X_{BMI} \rightarrow Y_{BP}$ at the mean of $Z_{LDL}$.
<<>>=
out2.2 <- lm(bp ~ bmi*zCen, data = dDat)
partSummary(out2.2, -c(1, 2))
@
The estimated slope for \texttt{bmi}, $\hat{\beta}_1 = 
\Sexpr{round(coef(out2.2)["bmi"], 3)}$, is the simple slope.
 
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}
  
  Test the simple slope of $X_{BMI} \rightarrow Y_{BP}$ at 1 $SD$ above the mean 
  of $Z_{LDL}$.
<<>>=
out2.3 <- lm(bp ~ bmi*zHi, data = dDat)
partSummary(out2.3, -c(1, 2))
@
The estimated slope for \texttt{bmi}, $\hat{\beta}_1 = 
\Sexpr{round(coef(out2.3)["bmi"], 3)}$, is the simple slope.
 
\end{frame}

%------------------------------------------------------------------------------%

\captionsetup{labelformat=empty}
\begin{frame}{Compare Approaches}
  
  The manual and the centering approaches give identical answers, barring 
  rounding errors: 
  \vb
<<echo = FALSE, results = "asis">>=
## Specify function to compute simple slopes:
getSS <- function(z, lmOut) {
    tmp <- coef(lmOut)
    tmp[2] + tmp[4]*z
}
##
## Specify function to compute SE for simple slopes:
getSE <- function(z, lmOut) {
    tmp <- vcov(lmOut)
    varB1 <- tmp[2, 2]
    varB3 <- tmp[4, 4]
    covB13 <- tmp[4, 2]

    sqrt(varB1 + 2 * z * covB13 + z^2 * varB3)
}

## Compute vector of simple slopes:
ssVec <- sapply(c(zMean - zSD, zMean, zMean + zSD),
                FUN   = getSS,
                lmOut = out2)
##
## Compute vector of SEs for simple slopes:
seVec <- sapply(c(zMean - zSD, zMean, zMean + zSD),
                FUN   = getSE,
                lmOut = out2)

ssVec2 <- c(coef(out2.1)["bmi"],
            coef(out2.2)["bmi"],
            coef(out2.3)["bmi"])

seVec2 <- c(sqrt(diag(vcov(out2.1)))["bmi"],
            sqrt(diag(vcov(out2.2)))["bmi"],
            sqrt(diag(vcov(out2.3)))["bmi"])

ssMat <- rbind(ssVec, ssVec2)
seMat <- rbind(seVec, seVec2)

colnames(ssMat) <- colnames(seMat) <- c("Z Low", "Z Center", "Z High")
rownames(ssMat) <- rownames(seMat) <- c("Manual", "Centering")

ssTab <- xtable(ssMat, 
                align   = "rccc",
                caption = "Simple Slopes",
                digits  = 6)
seTab <- xtable(seMat, 
                align   = "rccc",
                caption = "Standard Errors",
                digits  = 6)

print(ssTab, booktabs = TRUE)
@ 

\vx{-6}

<<echo = FALSE, results = "asis">>=
print(seTab, booktabs = TRUE)
@

\end{frame}
\captionsetup{labelformat=default}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Alternative Probing Strategies}
  
  Simple slopes analysis is nice due to its simplicity and ease of
  interpretation, but the $Z$ values we choose are totally arbitrary.
  \vc
  \begin{itemize}
  \item We may be missing important nuances that occur in areas of $Z$'s 
    distribution that we \emph{did not} pick.
  \end{itemize}
  \vb
  \pause
  The \emph{Johnson-Neyman} technique is an alternative approach that removes 
  the arbitrary selection of conditional $Z$ values.
  \vc
  \begin{itemize}
  \item Johnson-Neyman finds the \emph{region of significance} wherein the 
    conditional effect of $X$ on $Y$ is statistically significant.
    \vc
  \item Johnson-Neyman inverts the logic of simple slopes analysis to find what 
    cut-points on the moderator correspond to a critical $t$ value for the 
    conditional effect.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Johnson-Neyman Technique}
  
  With simple slopes analysis, we:
  \vb
  \begin{enumerate}
    \item Choose a conditional value of $Z$, say $Z_1$.
      \vb
    \item Calculate the simple slope, $\hat{f}(Z_1)$, and standard error, 
      $SE_{f(Z_1)}$, associated with $Z_1$.
      \vb
    \item Test $\hat{f}(Z_1)$ for significance via a simple Wald-type test:
      \begin{align}
        \hat{t} = \frac{\hat{f}(Z_1)}{SE_{f(Z_1)}} \label{waldEq}
      \end{align}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Johnson-Neyman Technique}
  
  With Johnson-Neyman, we:
  \vb
  \begin{enumerate}
  \item Choose an $\alpha$ level for our test.
    \begin{itemize}
    \item Use $\{\alpha, df\}$ to define a critical $t$-value
      \begin{itemize}
      \item E.g., $\alpha = 0.05$ and $df = 438$ $\Rightarrow$ $t_{crit} = 1.97$
      \end{itemize}
    \end{itemize}
    \vb
  \item Replace $\hat{t}$ in Equation \ref{waldEq} with $t_{crit}$ and re-arrange 
    into the following quadratic form:
    \begin{align}
      t_{crit}^2 SE_{f(Z)}^2 - \hat{f}^2(Z) = 0 \label{quadEq}
    \end{align}
  \item Solve Equation \ref{quadEq} to find the two values of $Z$ (i.e., the 
    roots) that produce critical $t$ statistics for the simple slope.
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Deriving Equation \ref{quadEq}}
  
  Recall the definitions of $\hat{f}(Z)$ and $SE_{f(Z)}$:
  \begin{align*}
    \hat{f}(Z) &= \hat{\beta}_1 + \hat{\beta}_3Z,\\[10pt]
    SE_{f(Z)} &= \sqrt{SE_{\beta_1}^2 + 2Z \cdot \text{COV}(\beta_1, \beta_3) + Z^2 SE_{\beta_3}^2}.
  \end{align*}
  So, our starting point is:
  \begin{align}
    t_{crit} = \frac{\hat{\beta}_1 + \hat{\beta}_3Z}{\sqrt{SE_{\beta_1}^2 + 2Z \cdot \text{COV}(\beta_1, \beta_3) + Z^2 SE_{\beta_3}^2}}. \label{tEq0}
  \end{align}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Deriving Equation \ref{quadEq}}
  
  Square both sides of Equation \ref{tEq0} to get:
  \begin{align}
    t_{crit}^2 = \frac{\hat{\beta}_1^2 + 2\hat{\beta}_1\hat{\beta}_3Z + \hat{\beta}_3^2Z^2}{SE_{\beta_1}^2 + 2Z \cdot \text{COV}(\beta_1, \beta_3) + Z^2 SE_{\beta_3}^2}. \label{t2Eq}
    \end{align}
  Lastly, move all terms in Equation \ref{t2Eq} to the left-hand side and 
  simplify to get the final quadratic form:
  \begin{align*}
    \left( t_{crit}^2 SE_{\beta_3}^2 - \hat{\beta}_3^2 \right) Z^2 +
    \left( 2t_{crit}^2 \text{COV}(\beta_1, \beta_3) - 2 \hat{\beta}_1 \hat{\beta}_3 \right) Z +
    \left( t_{crit}^2 SE_{\beta_1}^2 - \hat{\beta}_1^2 \right) = 0.
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Solving Equation \ref{quadEq}}
  
  Given our final quadratic equation:
  \begin{align*}
    \left( t_{crit}^2 SE_{\beta_3}^2 - \hat{\beta}_3^2 \right) Z^2 +
    \left( 2t_{crit}^2 \text{COV}(\beta_1, \beta_3) - 2 \hat{\beta}_1 \hat{\beta}_3 \right) Z +
    \left( t_{crit}^2 SE_{\beta_1}^2 - \hat{\beta}_1^2 \right) = 0,
  \end{align*}
  we find our solutions by taking:
  \begin{align*}
    a &= t_{crit}^2 SE_{\beta_3}^2 - \hat{\beta}_3^2,\\[8pt]
    b &= 2t_{crit}^2 \text{COV}(\beta_1, \beta_3) - 2 \hat{\beta}_1 \hat{\beta}_3,\\[8pt]
    c &= t_{crit}^2 SE_{\beta_1}^2 - \hat{\beta}_1^2,
  \end{align*}
  and applying the quadratic formula:
  \begin{align*}
    x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}.
  \end{align*}
  
\end{frame}
    
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
  
<<>>=
## Estimate model:
out <- lm(bp ~ bmi*ldl, data = dDat)

## Get (squared) critical value of t:
t2 <- qt(0.975, df = out$df.residual)^2

## Extract pertinent elements from the asymptotic 
## covariance matrix:
aCov <- vcov(out)
v1   <- diag(aCov)["bmi"]
v3   <- diag(aCov)["bmi:ldl"]
cv   <- aCov["bmi", "bmi:ldl"]

## Extract pertinent slope coefficients:
b1 <- coef(out)["bmi"]
b3 <- coef(out)["bmi:ldl"]
@ 

\pagebreak

<<>>=
## Compute coefficients of the quadratic equation:
a <- as.numeric(t2 * v3 - b3^2)
b <- as.numeric(2 * t2 * cv - 2 * b1 * b3)
c <- as.numeric(t2 * v1 - b1^2)

## Compute roots:
myRoots <- c(
(-b + sqrt(b^2 - 4 * a * c)) / (2 * a),
(-b - sqrt(b^2 - 4 * a * c)) / (2 * a)
)

myRoots
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpreting Johnson-Neyman}
  
  The roots produced by the Johnson-Neyman technique delineate the
  \emph{region of significance}.  
  \vb
  \begin{itemize}
  \item The conditional effect of $X$ on $Y$ is either significant everywhere 
    inside or outside of the interval defined by these two points.  
    \vb
  \item If only one of the points falls within the observed range of $Z$, ignore 
    the other point. 
    \vc
    \begin{itemize}
    \item In this case, the region of significance is either everywhere above or 
      everywhere below the legal root.
    \end{itemize}
    \vb
  \item If neither of the roots fall within the observed range of $Z$, then you 
    are in one of two cases:
    \vc
    \begin{enumerate}
    \item The focal effect is significant across the entire range of $Z$. 
      \vc
    \item The focal effect is not significant anywhere within the range of $Z$.
    \end{enumerate}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Interpreting Johnson-Neyman}
  
<<>>=
myRoots
partSummary(out2, -c(1, 2))
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Perspectives on Simple Slopes}
  
  Recall the formula for a simple slope:
  \begin{align*}
    f(Z) = \beta_1 + \beta_3Z
  \end{align*}
  \vc We can think about working with $f(Z)$ in, at least, two different ways:
  \vb
  \begin{enumerate}
  \item Treat $f(Z)$ as a weight for $X$ that we can use to evaluate the effect 
    of $X$ on $Y$ at different (discrete) levels of $Z$.
    \vb
  \item Consider how $f(Z)$, itself, changes as $Z$ (continuously) changes.  
  \end{enumerate}
  \vb
  The first option gives rise to simple slopes analysis while the second 
  embodies the spirit of Johnson-Neyman.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Confidence Bands}
  
  A natural quantity to consider is a confidence interval for $\hat{f}(Z)$:
  \begin{align*}
    CI = \hat{f}(Z) \pm t_{crit} \times SE_{f(Z)}
  \end{align*}
  \vx{-16}
  \begin{itemize}
  \item We can easily computed such intervals for the interesting values of $Z$ 
    that we chose for the simple slopes analysis.
  \end{itemize}
  \pause
  \vb
  When doing Johnson-Neyman, we can compute the pointwise values of the $CI$ for
  the entire range of $Z$.  
  \vb
  \begin{itemize}
  \item Interpolating between these pointwise CIs produces \emph{confidence 
    bands} for $\hat{f}(Z)$.
    \vc
    \begin{itemize}
    \item With these confidence bands, we can immediately check any value of $Z$ 
      for a significant simple slope.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%
  
\begin{frame}[fragile, allowframebreaks]{Example}
  
  Implementing the Johnson-Neyman technique by hand is a pain, but we can easily 
  do so by using the \textbf{rockchalk} package in \textsf{R}.\\ 
  
  \vb
  
<<message = FALSE, out.width = "65%">>=
library(rockchalk)

## First we need to create a 'plotSlopes' object:
plotOut <- plotSlopes(model      = out2,
                      plotx      = "bmi",
                      modx       = "ldl",
                      plotPoints = FALSE)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  We conduct the Johnson-Neyman test by using the \texttt{testSlopes} function 
  to modify the ``plotSlopes'' object:
  
<<>>= 
## Implement the J-N test:
wrap(testOut <- testSlopes(plotOut))

## Extract the significance boundaries:
testOut$jn$roots
@

\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}[fragile]{Example}

  \begin{columns}
    \begin{column}{0.35\textwidth}
      
      Finally, we can plot the result:
      
<<eval = FALSE>>= 
plot(testOut)
@

\end{column}
\begin{column}{0.65\textwidth}
  
<<echo = FALSE, results = "asis">>=
plot(testOut)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\captionsetup{labelformat = empty}
\begin{frame}{Compare Implementations}
  
  As expected, we get the same answers from our manual solution and from the 
  \textbf{rockchalk} implementation.
  
<<echo = FALSE, results = "asis">>=
tab <- matrix(c(myRoots, testOut$jn$roots), 
              ncol     = 2, 
              byrow    = TRUE,
              dimnames = list(c("Manual", "Rockchalk"), c("Low", "High"))
              )
print(xtable(tab, caption = "Johnson-Neyman Roots"), 
      booktabs = TRUE, 
      digits   = 3)
@ 

\end{frame}
\captionsetup{labelformat = default}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Categorical Moderators}
  
  Categorical moderators encode \emph{group-specific} effects.
  \vb
  \begin{itemize}
  \item E.g., if we include \emph{sex} as a moderator, we are modeling separate
    focal effects for males and females.
  \end{itemize}
  \va 
  Given a set of codes representing our moderator, we specify the
  interactions as before:
  \begin{align*}
    Y_{total} &= \beta_0 + \beta_1 X_{inten} + \beta_2 Z_{male} + 
    \beta_3 X_{inten}Z_{male} + \varepsilon\\\\
    Y_{total} &= \beta_0 + \beta_1 X_{inten} + \beta_2 Z_{lo} + \beta_3 Z_{mid} + 
    \beta_4 Z_{hi}\\ 
    &+ \beta_5 X_{inten}Z_{lo} + \beta_6 X_{inten}Z_{mid} + \beta_7 X_{inten}Z_{hi} + 
    \varepsilon
  \end{align*}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example}
  
<<>>=
## Load data:
socSup <- readRDS(paste0(dataDir, "social_support.rds"))

## Focal effect:
out3 <- lm(bdi ~ tanSat, data = socSup)
partSummary(out3, -c(1, 2))
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Estimate the interaction:
out4 <- lm(bdi ~ tanSat * sex, data = socSup)
partSummary(out4, -c(1, 2))
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  On the last slide, the estimated slope for \texttt{tanSat}, $\hat{\beta}_1 = 
  \Sexpr{round(coef(out4)["tanSat"], 3)}$, is the simple slope for females.
  \vc
  \begin{itemize}
  \item To estimate the simple slope for males, we simply change the reference 
    group of the \texttt{sex} factor and re-estimate the model.
  \end{itemize}
  
<<>>=
## Test the 'male' simple slope by changing reference group:
socSup$sex2 <- relevel(socSup$sex, ref = "male")

## Re-estimate the interaction:
out5 <- lm(bdi ~ tanSat * sex2, data = socSup)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
partSummary(out5, -c(1, 2))
@ 

The estimated slope for \texttt{tanSat}, $\hat{\beta}_1 = 
\Sexpr{round(coef(out5)["tanSat"], 3)}$, is now the simple slope for males.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Visualizing Categorical Moderation}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      {\scriptsize
        \vx{-12}
        \begin{align*}
          \hat{Y}_{BDI} &= \Sexpr{sprintf('%.2f', round(coef(out4)[1], 2))} 
            \Sexpr{sprintf('%.2f', round(coef(out4)[2], 2))} X_{tsat} + 
              \Sexpr{sprintf('%.2f', round(coef(out4)[3], 2))} Z_{male}\\ 
                &\Sexpr{sprintf('%.2f', round(coef(out4)[4], 2))} 
                  X_{tsat} Z_{male}
        \end{align*}
        \vx{-12}
      }
<<echo = FALSE, warning = FALSE>>=
out66 <- lm(BDI ~ tangiblesat + gender, data = socsupport)

p3 <- ggplot(data    = socsupport, 
             mapping = aes(x = tangiblesat, y = BDI, colour = gender)) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16))

p4 <- p3 + geom_jitter(na.rm = TRUE) +
    scale_colour_manual(values = c("red", "blue"))

p4 + geom_abline(slope     = coef(out4)["tanSat"],
                 intercept = coef(out4)[1],
                 colour    = "red",
                 size      = 1.5) +
    geom_abline(slope     = coef(out5)["tanSat"],
                intercept = coef(out5)[1],
                colour    = "blue",
                size      = 1.5) + 
    ggtitle("Moderation by Gender") +
    xlab("Tangible Satisfaction") +
    theme(plot.title = element_text(hjust = 0.5, size = 20, face = 2))
@ 

\end{column}

\begin{column}{0.5\textwidth}
  {\scriptsize
    \begin{align*}
      \hat{Y}_{BDI} = \Sexpr{sprintf('%.2f', round(coef(out66)[1], 2))}  
      \Sexpr{sprintf('%.2f', round(coef(out66)[2], 2))} X_{tsat}  
      \Sexpr{sprintf('%.2f', round(coef(out66)[3], 2))} Z_{male}
    \end{align*}
    \vx{-6}
  }
<<echo = FALSE>>=
p4 + geom_abline(slope     = coef(out66)["tangiblesat"],
                 intercept = coef(out66)[1],
                 colour    = "red",
                 size      = 1.5) +
    geom_abline(slope     = coef(out66)["tangiblesat"],
                intercept = (coef(out66)[1] + coef(out66)["gendermale"]),
                colour    = "blue",
                size      = 1.5) +
    ggtitle("Additive Gender Effect") +
    xlab("Tangible Satisfaction") +
    theme(plot.title = element_text(hjust = 0.5, size = 20, face = 2))
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Polynomial Regression}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Polynomial Regression}
  
  Polynomial regression simply incorporates powered transformations of the
  predictors into the model.
  \vb
  \begin{itemize}
  \item Polynomial terms (i.e., power terms) model curvature in the
    relationships.
  \end{itemize}
  \va
  We can think about polynomial terms as interactions between a predictor and 
  itself.
  \vb
  \begin{itemize}
  \item Many of the rules that apply to interactions transfer directly to
    polynomials.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Polynomial Visualization}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      We may hypothesize a curvilinear relationship between $X$ and $Y$.
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p5 <- ggplot(data = Cars93, mapping = aes(x = Horsepower, y = MPG.city)) +
    theme_classic()
p6 <- p5 + geom_point() +
    theme(text = element_text(family = "Courier", size = 16))
p6 + xlab("Horsepower") + ylab("MPG") + ylim(c(10, 50))
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Polynomial Visualization}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Polynomials are one way to model curvilinear relationships.
      \vb
      \begin{itemize}  
      \item {\color{blue}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp}$}
        \vb
      \item {\color{red}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} + 
        \hat{\beta}_2 X_{hp}^2$}
        \vb
      \item {\color{violet}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} + 
        \hat{\beta}_2 X_{hp}^2 + \hat{\beta}_3 X_{hp}^3$}
      \end{itemize}
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p6 + geom_smooth(method = "lm", formula = y ~ x, se = FALSE) + 
    geom_smooth(method  = "lm", 
                formula = y ~ x + I(x^2), 
                se      = FALSE, 
                colour  = "red") +
    geom_smooth(method  = "lm", 
                formula = y ~ x + I(x^2) + I(x^3), 
                se      = FALSE, 
                colour  = "purple") +
    xlab("Horsepower") + 
    ylab("MPG") +
    ylim(c(10, 50))
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<>>=
## Attach the data:
data(Cars93)

## Fit the linear model:
out6 <- lm(MPG.city ~ Horsepower, data = Cars93)

## Fit the quadratic model:
out7 <- lm(MPG.city ~ Horsepower + I(Horsepower^2), 
           data = Cars93)
@ 

\pagebreak

<<>>=
partSummary(out6, -c(1, 2))
@ 
  
\begin{itemize}
\item For each unit increase in horsepower, the expected change in fuel economy 
  is $\hat{\beta}_1 = \Sexpr{round(coef(out6)["Horsepower"], 4)}$ units.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
partSummary(out7, -c(1, 2))
@ 
\vx{-6}
\begin{itemize}
\item Extrapolating from powerless cars, each unit increase in 
  horsepower, is expected to change fuel economy by $\hat{\beta}_1 = 
  \Sexpr{round(coef(out7)["Horsepower"], 4)}$ units.
  \vc
\item For a unit increase in horsepower, the effect of horsepower on fuel 
  economy is expected to increase by $\hat{\beta}_2 = 
  \Sexpr{round(coef(out7)["I(Horsepower^2)"], 6)}$ units.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

Does adding the quadratic term explain a significantly larger proportion of
variability?
<<>>=
anova(out6, out7)
@ 

\pagebreak

Does adding the quadratic term reduce prediction error?
<<>>=
cv.lm(data   = Cars93, 
      models = c(formula(out6), formula(out7)), 
      K      = 5,
      names  = c("line", "quad")
      )
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Effects of Centering}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
cenPlot8(center = 0, xLab = "Horsepower")
@

\end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
cenPlot8(center = mean(Cars93$Horsepower), xLab = "Centered Horsepower")
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%
    
\begin{frame}{Effects of Centering}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
cenPlot8(center = 0, xLab = "Horsepower", linSlope = TRUE)
@

\end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
cenPlot8(center   = mean(Cars93$Horsepower), 
         xLab     = "Centered Horsepower", 
         linSlope = TRUE)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Mean center horsepower:
Cars93$HorsepowerMC <- 
    with(Cars93, Horsepower - mean(Horsepower))

## Fit the quadratic model:
out8 <- lm(MPG.city ~ HorsepowerMC + I(HorsepowerMC^2), 
           data = Cars93)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
partSummary(out8, -c(1, 2))
@ 
\vx{-6}
\begin{itemize}
\item Averaging over cars, each unit increase in horsepower, is expected to 
  change fuel economy by $\hat{\beta}_1 = 
  \Sexpr{round(coef(out8)["HorsepowerMC"], 4)}$ units.
  \vc
\item For a unit increase in horsepower, the effect of horsepower on fuel 
  economy is expected to increase by $\hat{\beta}_2 = 
  \Sexpr{round(coef(out8)["I(HorsepowerMC^2)"], 6)}$ units.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Choosing an ``Optimal'' Polynomial Order}
  
  We can use K-fold cross-validation to estimate which polynomial order will 
  give us the lowest prediction error.
<<>>=
cv.lm(data   = Cars93, 
      models = polyList(y = "MPG.city", 
                        x = "Horsepower", 
                        n = 5),
      K      = 5,
      names  = c("line", "quad", "cube", "quart", "quint")
      )
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualize the Best Model}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      The cubic model produced the lowest cross-validation error.
      \vc
      \begin{itemize}
      \item $\hat{Y}_{mpg} = +\hat{\beta}_1 X_{hp} + \hat{\beta}_2 X_{hp}^2 + \hat{\beta}_3 X_{hp}^3$
      \item We should be cautious.
        \vc
        \begin{itemize}
        \item We have very few data.
          \vc
        \item The few cases in the tails are strongly influencing the curve.
        \end{itemize}
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p6 + geom_smooth(method  = "lm", 
                 formula = y ~ x + I(x^2) + I(x^3), 
                 se      = FALSE, 
                 colour  = "blue") +
    xlab("Horsepower") + 
    ylab("MPG") +
    ylim(c(10, 50))
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Conclusion}
  
  \begin{itemize}
  \item \emph{Moderation} is a specific type of hypothesis.
    \vc
    \begin{itemize}
    \item The effect of one variable on the outcome (i.e., the focal effect) 
      changes as a function of some third variable (i.e., the moderator).
    \end{itemize}
    \vb
  \item We test for significant moderation by modeling \emph{interaction} terms.
    \vc
    \begin{itemize}
    \item If the slope associated with an interaction term is statistically 
      significant, we infer significant moderation.
    \end{itemize}
    \vb
  \item A significant interaction term only tells us that the focal effect is 
    dependent upon the moderator.
    \vc
    \begin{itemize}
    \item We must usually probe significant interactions to get any real insight 
      into the patterns of moderation.
    \end{itemize}
    
    \pagebreak
    
  \item We can probe an interaction by testing specific simple slopes.
    \vc
    \begin{itemize}
    \item \emph{Simple slopes} analysis
    \end{itemize}
    \vb
  \item We can also probe an interaction by finding the region of the 
    moderator's distribution wherein the focal effect is significant.
    \vc
    \begin{itemize}
    \item \emph{Johnson-Neyman} approach
    \end{itemize}
    \vb
  \item Categorical moderators imply group-specific focal effects.
    \vc
    \begin{itemize}
    \item We use simple slopes analysis to probe categorical moderation.
      \vc
    \item Johnson-Neyman doesn't make any sense with discrete moderators.
    \end{itemize}
  
  \pagebreak
  
  \item When we include interactions in our model, the first-order effects of 
    variables involved in the interactions become \emph{conditional effects}.
    \vc
    \begin{itemize}
    \item Conditional effects must be interpreted at the point where the other 
      predictor involved in the interaction is equal to zero.
    \end{itemize}
    \vb
  \item We can use the interpretation of conditional effects to our advantage 
    when testing simple slopes.
    \vc
    \begin{itemize}
    \item Center the moderator on a conditional value and re-run the model.
      \vc
    \item The estimated focal effect is a simple slope.
    \end{itemize}
  
    \pagebreak
    
  \item Polynomial regression models curvilinear trends.
    \vc
    \begin{itemize}
    \item Each unit increase in the order of the polynomial adds another bend to
      the curve.
    \end{itemize}
    \vb
  \item We can think of polynomials as interactions between the focal predictor 
    and itself.
    \vc
    \begin{itemize}
    \item Centering the focal predictors is especially important for 
      interpretation in polynomial regression.
    \end{itemize}
    \vb
  \item We need to be very cautious and constantly watch for overfitting when 
    using polynomial regression.
    \vc
    \begin{itemize}
    \item Even proper cross-validation may not protect us with small samples.
    \end{itemize}
  \end{itemize}
  
\end{frame}
  
%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtexFiles/statMethRefs.bib}

\end{frame}

\end{document}

