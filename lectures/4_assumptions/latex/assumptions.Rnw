%%% Title:    Regression in R 6: Assumptions & Diagnostics
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-12
%%% Modified: 2024-01-24

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{relsize}

\newcommand{\eqit}[1]{\textrm{\textit{#1}}}

\title{Assumptions \& Diagnostics}
\subtitle{Utrecht University Winter School: Regression in R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)
library(dplyr)
library(car)

source("../../../code/support/supportFunctions.R")

data(Cars93)

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/assumptions-",
               message = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t, plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\section{Assumptions of Linear Regression}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Assumptions of MLR}

  The assumptions of the linear model can be stated as follows:
  \vb
  \begin{enumerate}
  \item The model is linear in the parameters.
    \vc
    \begin{itemize}
    \item This is OK: $Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \beta_4X^2 + \beta_5X^3 + \varepsilon$
      \vc
    \item This is not: $Y = \beta_0 X^{\beta_1} + \varepsilon$
    \end{itemize}
    \vb
  \item The predictor matrix is \emph{full rank}.
    \vc
    \begin{itemize}
    \item $N > P$
      \vc
    \item No $X_p$ can be a linear combination of other predictors.
    \end{itemize}

    \pagebreak

  \item The predictors are strictly exogenous.\label{exo}
    \vc
    \begin{itemize}
    \item The predictors do not correlated with the errors.
      \vc
    \item $\textrm{Cov}(\hat{Y}, \varepsilon) = 0$
      \vc
    \item $\textrm{E}[\varepsilon_n] = 0$
    \end{itemize}
    \vb
  \item The errors have constant, finite variance.\label{constVar}
    \vc
    \begin{itemize}
    \item $\textrm{Var}(\varepsilon_n) = \sigma^2 < \infty$
    \end{itemize}
    \vb
  \item The errors are uncorrelated.\label{indErr}
    \vc
    \begin{itemize}
    \item $\textrm{Cov}(\varepsilon_i, \varepsilon_j) = 0, ~ i \neq j$
    \end{itemize}
    \vb
  \item The errors are normally distributed.\label{normErr}
    \vc
    \begin{itemize}
    \item $\varepsilon \sim \textrm{N}(0, \sigma^2)$
    \end{itemize}
  \end{enumerate}

  \pagebreak

  The assumption of \emph{spherical errors} combines Assumptions \ref{constVar}
  and \ref{indErr}.
  \begin{align*}
    \textrm{Var}(\varepsilon) =
    \begin{bmatrix}
      \sigma^2 & 0 & \cdots & 0\\
      0 & \sigma^2 & \cdots & 0\\
      0 & 0 & \ddots & 0\\
      0 & 0 & \cdots & \sigma^2
    \end{bmatrix} =
    \sigma^2\mathbf{I}_N
  \end{align*}
  We can combine Assumptions \ref{exo}, \ref{constVar}, \ref{indErr}, and
  \ref{normErr} by assuming independent and identically distributed normal
  errors:
  \begin{itemize}
  \item $\varepsilon \overset{iid}{\sim} \textrm{N}(0, \sigma^2)$
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Violating Assumptions}

  \begin{enumerate}
  \item If the model is not linear in the parameters, then we're not even
    working with linear regression.
    \begin{itemize}
    \item We need to move to entirely different modeling paradigm.
    \end{itemize}
    \vb
  \item If the predictor matrix is not full rank, the model is not estimable.
    \begin{itemize}
    \item The parameter estimates cannot be uniquely determined from the data.
    \end{itemize}
    \vb
  \item If the predictors are not exogenous, the estimated regression
    coefficients will be biased.
    \vb
  \item If the errors are not spherical, the standard errors will be biased.
    \begin{itemize}
    \item The estimated regression coefficients will be unbiased, though.
    \end{itemize}
    \vb
  \item If errors are non-normal, small-sample inferences may be biased.
    \begin{itemize}
    \item The justification for some tests and procedures used in regression
      analysis may not hold.
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\section{Regression Diagnostics}

%------------------------------------------------------------------------------%

\begin{frame}{Regression Diagnostics}

  If some of the assumptions are (grossly) violated, the inferences we make
  using the model may be wrong.
  \begin{itemize}
  \item We need to check the tenability of our assumptions before leaning too
    heavily on the model estimates.
  \end{itemize}
  \vb
  These checks are called \emph{regression diagnostics}.
  \begin{itemize}
  \item Graphical visualizations
    \vc
  \item Quantitative indices/measures
    \vc
  \item Formal statistical tests
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Residual Plots}

  Plots of residuals vs. predicted values are very useful.
  \begin{itemize}
  \item Here we see clear evidence of heteroscedasticity.
  \end{itemize}
  
<<>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
@

\begin{columns}
\begin{column}{0.5\textwidth}
 
<<echo = FALSE, message = FALSE>>=
p1 <- gg0(x = Cars93$Horsepower, y = Cars93$Price)
p1 + geom_smooth(method = "lm", se = FALSE) +
    xlab("Horsepower") +
    ylab("Price")
@
\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 <- gg0(x = predict(out1), y = resid(out1))
p1 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted Price") +
    ylab("Residual Price")
@
  
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Scale-Location Plots}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}

      Scale-location plots also offer an excellent means of detecting 
      non-constant error variance.
      \vc
      \begin{itemize}
        \item Plot an approximation of the pointwise residual variance against 
          the fitted values.
          \vc
        \item Any trend indicates systematic changes in the residual variance.
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
     
<<>>=
plot(out1, 3)
@
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual Plots}
  
  Residual plots can also show violations of the linearity assumption.
  
<<>>=
out2 <- lm(MPG.highway ~ Horsepower, data = Cars93)
@ 

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, message = FALSE>>=
p4 <- gg0(x = Cars93$Horsepower, y = Cars93$MPG.highway)
p5 <- p4 + geom_smooth(method = "lm", se = FALSE) +
    xlab("Horsepower") +
    ylab("MPG")
p5
@

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
p2 <- gg0(x = predict(out2), y = resid(out2))
p3 <- p2 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted MPG") +
    ylab("Residual MPG")

p3 + geom_smooth(method = "loess", se = FALSE)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual Plots}
  We can easily create residual plots from a fitted model object.
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<>>=
plot(out1, 1)
@

\end{column}
\begin{column}{0.5\textwidth}

<<>>=
plot(out2, 1)
@

\end{column}
\end{columns}
 
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Partial Residual Plots}
  
  In multiple linear regression, ordinary residual plots may not reveal 
  nonlinearity.
  \begin{itemize}
  \item If we do find nonlinearity, ordinary residual plots can't tell us which 
    term is causing the issue.
  \end{itemize}
  \vb
  Partial residual plots show the trend for individual predictors, after 
  controlling for all other variables in the model.
  \vc
  \begin{enumerate}
  \item First, define the partial residual for the \emph{p}th predictor.
    \begin{align*}
      \hat{\varepsilon}_n^{(p)} = \hat{\varepsilon}_n + \hat{\beta}_pX_{np}
    \end{align*}
  \item Then, plot the partial residuals, $\hat{\varepsilon}^{(p)}$, against
    $X_p$, for all predictors.
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Partial Residual Plots}

  Let's look at an example. Consider the following model.
  
<<>>=
out3 <- lm(MPG.highway ~ Horsepower + Turn.circle, data = Cars93)
partSummary(out3, -1)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Partial Residual Plots}

<<out.width = "55%">>=
plot(out3, 1)
@   

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Partial Residual Plots}

<<fig.asp = 0.5>>=
crPlots(out3, ylab = "Partial Residual")
@   

\end{frame}

%------------------------------------------------------------------------------%


\begin{frame}[fragile]{Partial Residual Plots}

  Let's add the quadratic expansion of Horsepower.
  
<<>>=
out4 <- update(out3, ". ~ . + poly(Horsepower, 2) - Horsepower")
partSummary(out4, -1)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Partial Residual Plots}

<<out.width = "55%">>=
plot(out4, 1)
@   

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Partial Residual Plots}

<<fig.asp = 0.5>>=
crPlots(out4, ylab = "Partial Residual")
@   

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{QQ-Plots}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}

      A normal Q-Q Plot is one of the best ways to evaluate the normality 
      assumption.
      \vc
      \begin{itemize}
      \item Plot the quantiles of the residual distribution against the
        theoretically ideal quantiles.
        \vc
      \item We can actually use a Q-Q Plot to compare any two distributions.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 2)
@

<<echo = FALSE, eval = FALSE>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
qqnorm(resid(out1))
qqline(resid(out1))
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Influential Observations}

%------------------------------------------------------------------------------%

\begin{frame}{Influential Observations}

  Influential observations contaminate analyses in two ways:
  \vc
  \begin{enumerate}
  \item Exert too much influence on the fitted regression model
    \vc
  \item Invalidate estimates/inferences by violating assumptions
  \end{enumerate}
  \vb
  There are two distinct types of influential observations:
  \vc
  \begin{enumerate}
  \item Outliers
    \vc
    \begin{itemize}
    \item Observations with extreme outcome values, relative to the other data.
      \vc
    \item Observations with outcome values that fit the model very badly.
    \end{itemize}
    \vb
  \item High-leverage observations
    \vc
    \begin{itemize}
    \item Observation with extreme predictor values, relative to other data.
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outliers}

  Outliers can be identified by scrutinizing the residuals.
  \vc
  \begin{itemize}
  \item Observations with residuals of large magnitude may be outliers.
    \vc
  \item The difficulty arises in quantifying what constitutes a ``large''
    residual.
  \end{itemize}
  \vb
  If the residuals do not have constant variance, then we cannot directly
  compare them.
  \vc
  \begin{itemize}
  \item We need to standardize the residuals in some way.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Studentized Residuals}

  Begin by defining the concept of a \emph{deleted residual}:
  \begin{align*}
    \hat{\varepsilon}_{(n)} = Y_n - \hat{Y}_{(n)}
  \end{align*}
  \vx{-18}
  \begin{itemize}
  \item $\hat{\varepsilon}_{(n)}$ quantifies the distance of $Y_n$ from the
    regression line estimated after excluding the $n$th observation.
  \end{itemize}
  \va
  If we standardize the deleted residual, $\hat{\varepsilon}_{(n)}$, we get the
  externally studentized residual:
  \begin{align*}
    t_{(n)} = \frac{\hat{\varepsilon}_{(n)}}{SE_{\hat{\varepsilon}_{(n)}}}
  \end{align*}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Studentized Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Index plots of the externally studentized residuals can help spotlight
      potential outliers.
      \vb
      \begin{itemize}
      \item Look for observations that clearly ``stand out from the crowd.''
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<>>=
plot(rstudent(out1))
@

<<echo = FALSE, eval = FALSE>>=
p1 <- gg0(x = c(1 : nrow(Cars93)), y = rstudent(out1))

p1 + geom_hline(yintercept = c(-3, 3), colour = "red") +
    xlab("Index") +
    ylab("Externally Studentized Residual MPG")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{High-Leverage Points}

  We identify high-leverage observations through their \emph{leverage} values.
  \vb
  \begin{itemize}
  \item An observation's leverage, $h_n$, quantifies the extent to which its
    predictors affect the fitted regression model.
    \vb
  \item Observations with $X$ values very far from the mean, $\bar{X}$, affect
    the fitted model disproportionately.
  \end{itemize}
  \vb
  \pause
  In simple linear regression, the $n$th leverage is given by:
  \begin{align*}
    h_n = \frac{1}{N} + \frac{\left(X_n - \bar{X}\right)^2}
    {\sum_{m = 1}^N \left(X_{m} - \bar{X}\right)^2}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Leverage Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Index plots of the leverage values can help spotlight high-leverage points.
      \vb
      \begin{itemize}
      \item Again, look for observations that clearly ``stand out from the
        crowd.''
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<>>=
plot(hatvalues(out1))
@

<<echo = FALSE, eval = FALSE>>=
n <- nrow(Cars93)

p1 <- gg0(x = c(1 : n), y = hatvalues(out1))

p1 + geom_hline(yintercept = 2 / n, colour = "red") +
    xlab("Index") +
    ylab("Leverage Values")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Outliers \& Leverages $\rightarrow$ Influential Points}

  Observations with high leverage or large (externally) studentized residuals
  are not necessarily influential.
  \vc
  \begin{itemize}
  \item High-leverage observations tend to be more influential than outliers.
    \vc
  \item The worst problems arise from observations that are both outliers and
    have high leverage.
  \end{itemize}
  \vb
  \emph{Measures of influence} simultaneously consider extremity in both $X$
  and $Y$ dimensions.
  \vc
  \begin{itemize}
  \item Observations with high measures of influence are very likely to cause
    problems.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Measures of Influence}

  Measures of influence come in two flavors.
  \vb
  \begin{enumerate}
  \item Global measures of influence
    \vc
    \begin{itemize}
    \item Cook's Distance
    \end{itemize}
    \vb
  \item Coefficient-specific measures of influence
    \vc
    \begin{itemize}
    \item DFBETAS
    \end{itemize}
  \end{enumerate}
  \vb
  All measures of influence use the same logic as the deleted residual.
  \vc
  \begin{itemize}
  \item Compare models estimated from the whole sample to models estimated from
    samples excluding individual observations.
  \end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Global Measures of Influence}

  Each observation gets a Cook's Distance value.
  \begin{align*}
    \textrm{Cook's} ~ D_n &= \frac{\sum_{n = 1}^N \left( \hat{Y}_n - \hat{Y}_{(n)} \right)^2}{\left(P + 1\right) \hat{\sigma}^2}\\[6pt]
                          &= (P + 1)^{-1} t_n^2 \frac{h_n}{1 - h_n}
  \end{align*}

  Each regression coefficient (including the intercept) gets a DFBETAS value for
  each observation.
  \begin{align*}
    \textrm{DFBETAS}_{np} = \frac{\hat{\beta}_p - \hat{\beta}_{p(n)}}{\textrm{SE}_{\hat{\beta}_{p(n)}}}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Plots of Cook's Distance}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Index plots of Cook's distances can help spotlight the influential points.
      \vb
      \begin{itemize}
      \item Look for observations that clearly ``stand out from the crowd.''
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, eval = FALSE>>=
fCrit <- qf(0.5, 2, n - 2)

p1 <- gg0(x = c(1 : n), y = cooks.distance(out1))

p1 + geom_hline(yintercept = c(fCrit), colour = "red") +
    xlab("Index") +
    ylab("Cook's Distance")
@

<<>>=
cd <- cooks.distance(out1)
plot(cd)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Plots of Cook's Distance}

  We can create Cook's Distance plots by plotting a fitted model object.

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<out.width = "90%">>=
plot(out1, 4)
@
      
    \end{column}
    \begin{column}{0.5\textwidth}

<<out.width = "90%">>=
plot(out4, 4)
@ 
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Plots of DFBETAS}

<<eval = FALSE>>=
dfb <- dfbetas(out1)
plot(dfb[ , 1], main = "Intercept")
plot(dfb[ , 2], main = "Slope")
@

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
dfb <- dfbetas(out1)
plot(dfb[ , 1], main = "Intercept")
@

<<echo = FALSE, eval = FALSE>>=
dfb <- dfbetas(out1)

p1 <- gg0(x = c(1 : n), y = dfb[ , 1])

p1 + geom_hline(yintercept = c(-2 / sqrt(n), 2 / sqrt(n)), colour = "red") +
    xlab("Index") +
    ylab("DFBETAS for Intercept")
@

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE>>=
plot(dfb[ , 2], main = "Slope")
@

<<echo = FALSE, eval = FALSE>>=
p1 <- gg0(x = c(1 : n), y = dfb[ , 2])

p1 + geom_hline(yintercept = c(-2 / sqrt(n), 2 / sqrt(n)), colour = "red") +
    xlab("Index") +
    ylab("DFBETAS for Slope")
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Influence Plots} 

  Plotting studentized residuals against leverages can help identify influential 
  cases.
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<out.width = "90%">>=
plot(out1, 5)
@
      
    \end{column}
    \begin{column}{0.5\textwidth}

<<out.width = "90%">>=
plot(out4, 5)
@ 
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Treating Influential Observations}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Removing Influential Observations}

<<>>=
(maxD <- which.max(cd))
@

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Observation number \Sexpr{maxD} was the most influential
      according to Cook's Distance.
      \vb
      \begin{itemize}
      \item Removing that observation has a small impact on the fitted
        regression line.
        \vc
      \item Influential observations don't only affect the regression line,
        though.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
x <- Cars93$Horsepower
y <- Cars93$Price

x2 <- x[-maxD]
y2 <- y[-maxD]

colVec <- rep("black", nrow(Cars93))
colVec[maxD] <- "red"

p1 <- gg0(x = x, y = y, points = FALSE) +
    geom_point(aes(colour = colVec), show.legend = FALSE, size = 2) +
    xlab("Horsepower") +
    ylab("Price")

p1 + geom_smooth(method = "lm", se = FALSE, linewidth = 2) +
geom_smooth(mapping   = aes(x = x2, y = y2),
            method    = "lm",
            se        = FALSE,
            color     = "red",
            linetype  = "dashed",
            linewidth = 2) +
scale_colour_manual(values = c("black", "red"))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Removing Influential Observations}

<<>>=
## Exclude the influential case:
Cars93.2 <- Cars93[-maxD, ]

## Fit model with reduced sample:
out2 <- lm(Price ~ Horsepower, data = Cars93.2)

summary(out1)$coefficients %>% round(6)
summary(out2)$coefficients %>% round(6)
@

\pagebreak

<<>>=
partSummary(out1, 2)
partSummary(out2, 2)
@

\pagebreak

<<>>=
summary(out1)[c("sigma", "r.squared", "fstatistic")] %>%
    unlist() %>%
    head(3)
summary(out2)[c("sigma", "r.squared", "fstatistic")] %>%
    unlist() %>%
    head(3)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Removing Influential Observations}

<<>>=
(maxDs <- sort(cd) %>% names() %>% tail(2) %>% as.numeric())
@

  \begin{columns}
    \begin{column}{0.5\textwidth}

      If we remove the two most influential observations, \Sexpr{maxDs[1]} and
      \Sexpr{maxDs[2]}, the fitted regression line barely changes at all.
      \vc
      \begin{itemize}
      \item The influences of these two observations were counteracting one
        another.
      \item We're probably still better off, though.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
x <- Cars93$Horsepower
y <- Cars93$Price

x2 <- x[-maxDs]
y2 <- y[-maxDs]

colVec <- rep("black", nrow(Cars93))
colVec[maxDs] <- "red"

p1 <- gg0(x = x, y = y, points = FALSE) +
    geom_point(aes(colour = colVec), show.legend = FALSE, size = 2) +
    xlab("Horsepower") +
    ylab("Price")

p1 + geom_smooth(method = "lm", se = FALSE, linewidth = 2) +
geom_smooth(mapping   = aes(x = x2, y = y2),
            method    = "lm",
            se        = FALSE,
            color     = "red",
            linetype  = "dashed",
            linewidth = 2) +
scale_colour_manual(values = c("black", "red"))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Removing Influential Observations}

<<>>=
## Exclude influential cases:
Cars93.2 <- Cars93[-maxDs, ]

## Fit model with reduced sample:
out2.2 <- lm(Price ~ Horsepower, data = Cars93.2)

summary(out1)$coefficients %>% round(6)
summary(out2.2)$coefficients %>% round(6)
@

\pagebreak

<<>>=
partSummary(out1, 2)
partSummary(out2.2, 2)
@

\pagebreak

<<>>=
summary(out1)[c("sigma", "r.squared", "fstatistic")] %>%
    unlist() %>%
    head(3)
summary(out2.2)[c("sigma", "r.squared", "fstatistic")] %>%
    unlist() %>%
    head(3)
@

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
