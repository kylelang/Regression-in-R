%%% Title:    Regression in R 2: Prediction
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2023-01-26

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{booktabs}

\title{Prediction}
\subtitle{Utrecht University Winter School: Regression in R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{2022-02-02}

%------------------------------------------------------------------------------%

\begin{document}

<<setup, include=FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)

source("../../../code/support/supportFunctions.R")

dataDir <- "../../../data/"

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/prediction-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction}

  So far, we've focused primarily on inferences about the estimated regression
  coefficients.
  \vb
  \begin{itemize}
  \item Asking questions about how $\mathbf{X}$ is related to $Y$
  \end{itemize}
  \vb
  We can also use linear regression for \emph{prediction}.
  \vb
  \begin{itemize}
  \item Given a new observation, $X_m$, what outcome value, $\hat{Y}_m$, does
    our model attribute to the $m$th observation?
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\section{Generating Predictions}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction Example}

<<echo = FALSE>>=
diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))

trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp + glu + bmi, data = trainDat)
b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

confInt <- round(
    predict(out1, newdat = testDat, interval = "confidence")[1, 2 : 3], 3
)
predInt <- round(
    predict(out1, newdat = testDat, interval = "prediction")[1, 2 : 3], 3
)
@

To fix ideas, let's reconsider the \emph{diabetes} data and the following model:
\begin{align*}
  Y_{LDL} = \beta_0 + \beta_1 X_{BP} + \beta_2 X_{gluc} + \beta_3 X_{BMI} +
  \varepsilon
\end{align*}
Training this model on the first $N = 400$ patients' data produces the following
fitted model:
\begin{align*}
  \hat{Y}_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP} + \Sexpr{b2} X_{gluc} +
  \Sexpr{b3} X_{BMI}
\end{align*}
\pause
Suppose a new patient presents with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and
$BMI = \Sexpr{x3}$. We can predict their $LDL$ score by:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2}
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(predict(out1, testDat[1 : 2, ])[1], 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction with Centered X Variables}

<<echo = FALSE>>=
diabetes$bp90   <- diabetes$bp - 90
diabetes$glu100 <- diabetes$glu - 100
diabetes$bmi30  <- diabetes$bmi - 30

trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp90 + glu100 + bmi30, data = trainDat)

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

x1.2 <- testDat[1, "bp90"]
x2.2 <- testDat[1, "glu100"]
x3.2 <- testDat[1, "bmi30"]

pred0 <- t(matrix(c(1, x1, x2, x3))) %*% matrix(coef(out1))
pred1 <- t(matrix(c(1, x1.2, x2.2, x3.2))) %*% matrix(coef(out1))
@

Suppose we fit the preceding model with $BP$ centered at 90, $gluc$ centered at
100, and $BMI$ centered at 30.
\begin{itemize}
\item We'd get the following fitted model:
\end{itemize}
\begin{align*}
  \hat{Y}_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP.90} + \Sexpr{b2} X_{gluc.100} + \Sexpr{b3} X_{BMI.30}
\end{align*}
\pause
Now, let's generate predictions for our patient with $BP = \Sexpr{x1}$,
$gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2}
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(pred0, 3)}
\end{align*}
\pause
To get the correct prediction, we need to plug-in the centered X values:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1} - 90) + \Sexpr{b2}
  (\Sexpr{x2} - 100) + \Sexpr{b3} (\Sexpr{x3} - 30)\\
  &= \Sexpr{round(pred1, 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Interval Estimates for Prediction}

%------------------------------------------------------------------------------%

\begin{frame}{Interval Estimates for Prediction}

  To quantify uncertainty in our predictions, we want to use an appropriate
  interval estimate.
  \vb
  \begin{itemize}
  \item Two flavors of interval are applicable to predictions:
    \begin{enumerate}
    \item Confidence intervals for $\hat{Y}$
      \vc
    \item Prediction intervals for a specific observation
    \end{enumerate}
    \vb
  \item CIs for $\hat{Y}$ give a likely range (in the sense of coverage
    probability and ``confidence'') for the true conditional mean of $Y$,
    $\mu_{Y|X^*}$.
    \begin{itemize}
    \item They only account for uncertainty in the estimated regression
      coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$.
    \end{itemize}
    \vb
  \item Prediction intervals give a likely range (in the same sense as CIs) for
    future outcome values, $Y^*$.
    \begin{itemize}
    \item They also account for the regression errors, $\varepsilon$.
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Confidence vs. Prediction Intervals}

 \begin{columns}
   \begin{column}{0.5\textwidth}

     Let's visualize the predictions from a simple model:
     \begin{align*}
       Y_{BP} = {\color{blue} \hat{\beta}_0 + \hat{\beta}_1 X_{BMI}} +
       {\color{red} \hat{\varepsilon}}
     \end{align*}
     \vx{-12}
     \begin{itemize}
     \item<2-3> CIs for $\hat{Y}$ ignore the errors, ${\color{red}\varepsilon}$.
       \begin{itemize}
       \item They only care about the best-fit line,
         ${\color{blue} \beta_0 + \beta_1 X_{BMI}}$.
       \end{itemize}
       \vb
     \item<3> Prediction intervals are wider than CIs.
       \begin{itemize}
       \item They account for the additional uncertainty contributed by
         ${\color{red}\varepsilon}$.
       \end{itemize}
     \end{itemize}

   \end{column}
   \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
out1 <- lm(bp ~ bmi, data = trainDat)

testDat$preds <- predict(out1, newdata = testDat)

ci <- predict(out1, newdata = testDat, interval = "confidence")[ , -1]
pi <- predict(out1, newdata = testDat, interval = "prediction")[ , -1]

colnames(ci) <- c("ciLo", "ciHi")
colnames(pi) <- c("piLo", "piHi")

testDat <- data.frame(testDat, ci, pi)

p1 <- ggplot(data = testDat, aes(x = bmi, y = bp)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) +
    geom_segment(aes(x = bmi, y = bp, xend = bmi, yend = preds),
                 color = "red") +
    geom_line(mapping = aes(x = bmi, y = preds), color = "blue", size = 1) +
    geom_point() +
    ylim(range(pi))
@
\only<1>{
<<echo = FALSE, cache = TRUE>>=
print(p1)
@
}
\only<2>{
<<echo = FALSE, cache = TRUE>>=
p2 <- p1 +
    geom_line(mapping = aes(x = bmi, y = ciLo), size = 1, linetype = "solid") +
    geom_line(mapping = aes(x = bmi, y = ciHi), size = 1, linetype = "solid")
p2
@
}
\only<3>{
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
p2 +
    geom_line(mapping = aes(x = bmi, y = piLo), size = 1, linetype = "dashed") +
    geom_line(mapping = aes(x = bmi, y = piHi), size = 1, linetype = "dashed")
@
}

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Confidence vs. Prediction Intervals}

  Assume we want predictions for a new observation, $X^*$. Then our intervals
  can be computed as follows:
  \vb
  \begin{itemize}
  \item Confidence interval:
    \begin{align*}
      CI = \hat{Y} \pm t_{crit} \sqrt{\frac{\hat{\sigma}^2}{N} + \frac{\left(X^* - \bar{X}\right)^2}{\left(\sum_{n = 1}^N X_n- \bar{X}\right)^2}}
    \end{align*}
  \item Prediction interval:
    \begin{align*}
      PI = \hat{Y} \pm t_{crit} \sqrt{{\color{red}\hat{\sigma}^2} + \frac{\hat{\sigma}^2}{N} + \frac{\left(X^* - \bar{X}\right)^2}{\left(\sum_{n = 1}^N X_n- \bar{X}\right)^2}}
    \end{align*}
  \end{itemize}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\begin{frame}{Interval Estimates Example}

  Going back to our hypothetical ``new'' patient, we get the following $95\%$
  interval estimates:
  \begin{align*}
    95\% CI_{\hat{Y}_{LDL}} &= [\Sexpr{confInt[1]}; \Sexpr{confInt[2]}]\\[8pt]
    95\% PI &= [\Sexpr{predInt[1]}; \Sexpr{predInt[2]}]
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item We can be 95\% confident that the \underline{average \emph{LDL}} of
    patients with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$
    will be somewhere between \Sexpr{confInt[1]} and \Sexpr{confInt[2]}.
    \vb
  \item We can be 95\% confident that the \underline{\emph{LDL} of a specific
    patient} with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$
    will be somewhere between \Sexpr{predInt[1]} and \Sexpr{predInt[2]}.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Specifying Predictive Models}

  When focused on inferences about regression coefficients, we care very much
  about the predictors entered into the model.
  \vb
  \begin{itemize}
  \item Partial regression coefficients must be interpreted as controlling for
    all other predictors.
  \end{itemize}
  \vb
  \pause
  When focused on prediction, we often don't care as much about the
  specific variables that enter the model.
  \vb
  \begin{itemize}
  \item We prefer whatever set of features produces the best predictive
    performance.
    \vb
  \item We may want to know which are the ``best'' predictors.
    \vc
    \begin{itemize}
    \item We usually want the data to ``give'' us this answer.
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{\textsc{Aside}: Polynomial Regression}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We may hypothesize a curvilinear relationship between $X$ and $Y$.
      \vc
      \begin{itemize}
      \item Polynomial regression adds powered transformations of the
        predictors into the model.
        \vc
      \item Polynomial terms (i.e., power terms) model curvature in the
        relationships.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p5 <- ggplot(data = Cars93, mapping = aes(x = Horsepower, y = MPG.city)) +
    theme_classic()
p6 <- p5 + geom_point() +
    theme(text = element_text(family = "Courier", size = 16))
p6 + xlab("Horsepower") + ylab("MPG") + ylim(c(10, 50))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{\textsc{Aside}: Polynomial Regression}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      Polynomials are one way to model curvilinear relationships.
      \vb
      \begin{itemize}
      \item {\color{blue}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp}$}
        \vb
      \item {\color{red}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} +
        \hat{\beta}_2 X_{hp}^2$}
        \vb
      \item {\color{violet}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} +
        \hat{\beta}_2 X_{hp}^2 + \hat{\beta}_3 X_{hp}^3$}
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p6 + geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
    geom_smooth(method  = "lm",
                formula = y ~ x + I(x^2),
                se      = FALSE,
                colour  = "red") +
    geom_smooth(method  = "lm",
                formula = y ~ x + I(x^2) + I(x^3),
                se      = FALSE,
                colour  = "purple") +
    xlab("Horsepower") +
    ylab("MPG") +
    ylim(c(10, 50))
@

\end{column}
\end{columns}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\section{Evaluating Predictive Models}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Evaluating Predictive Performance}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      How do we assess ``good'' prediction?
      \vb
      \begin{itemize}
      \item Can we simply find the model that best predicts the data used to
        train the model?
        \vb
      \item What are we trying to do when building a predictive model?
        \vb
      \item Can we quantify this objective with some type of fit measure?
      \end{itemize}

      \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
nObs <- 10
r2   <- 0.9

x   <- rangeNorm(rnorm(nObs), 18, 60)
eta <- 0.035 * x + 0.015 * x^2
sig <- (var(eta) / r2) - var(eta)
y   <- eta + rnorm(nObs, 0, sqrt(sig))

x2   <- rangeNorm(rnorm(nObs), 18, 60)
eta2 <- 0.035 * x2 + 0.015 * x2^2
sig2 <- (var(eta2) / r2) - var(eta2)
y2   <- eta2 + rnorm(nObs, 0, sqrt(sig2))

form1 <- as.formula(
    paste0("y ~ x + ", paste0("I(x^", c(2 : 10), ")", collapse = " + "))
)

p1 <- gg0(x = x, y = y) + xlab("Age") + ylab("Mortality Risk")
p1
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Different Possible Models}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth", message = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE)
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@

\end{column}
\end{columns}

  \va

\begin{columns}
  \begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2) + I(x^3))
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2) + I(x^3) + I(x^4))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Over-fitting}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We can easily go too far.
      \vb
      \begin{itemize}
      \item Enough polynomial terms will exactly replicate any data.
        \vb
      \item Is this what we're trying to do?
        \vb
      \item What kind of issues arise in the extreme case?
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = form1)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Over-fitting}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Should we be pleased to be able to perfectly predict mortality risk?
      \vb
      \begin{itemize}
      \item Is our model useful?
        \vc
      \item What happens if we try to apply our fitted model to new data?
      \end{itemize}

    \end{column}

    \pause

    \begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p2 <- gg0(x = x2, y = y2) + xlab("Age") + ylab("Mortality Risk")
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = form1)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Correct Fit}

  Let's try something a bit more reasonable.\\
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2)
                 )
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{A Sensible Goal}

  Our goal is to train a model that can best predict \emph{new data}.
  \vc
  \begin{itemize}
  \item The predictive performance on the training data is immaterial.
  \item We can always fit the training data arbitrarily well.
  \item Fit to the training data will always be at-odds with fit to future data.
  \end{itemize}
  \vc
  This conflict is the driving force behind the \emph{bias-variance trade-off}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Model Fit for Prediction}

  When assessing predictive performance, we will most often use the \emph{mean
    squared error} (MSE) as our criterion.
  \vb
  \begin{align*}
    MSE &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{Y}_n\right)^2\\
    &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{\beta}_0 -
    \sum_{p = 1}^P \hat{\beta}_p X_{np} \right)^2\\
    &= \frac{RSS}{N}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Training vs. Test MSE}

  The MSE on the preceding slide (i.e., the only MSE we've considered, so far)
  is computing entirely from training data.
  \vb
  \begin{itemize}
  \item \emph{Training MSE}
  \end{itemize}
  \vb
  What we want is a measure of fit to new, \emph{testing} data.
  \vb
  \begin{itemize}
  \item \emph{Test MSE}
    \vb
  \item Given $M$ new observations $\{Y_m, X_{m1}, X_{m2}, \ldots, X_{mP}\}$,
    and a fitted regression model, $f(\mathbf{X})$, defined by the
    coefficients $\{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \ldots,
    \hat{\beta}_P\}$, the \emph{Test MSE} is given by:
    \begin{align*}
      MSE &= \frac{1}{M} \sum_{m = 1}^M \left(Y_m - \hat{\beta}_0 -
      \sum_{p = 1}^P \hat{\beta}_p X_{mp} \right)^2\\
    \end{align*}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Training vs. Test MSE}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      \textcolor{red}{Training MSE} will always decrease in response to
      increased model complexity.
      \vb
      \begin{itemize}
      \item Note the red line in the plot.
      \end{itemize}
      \vb
      \textcolor{blue}{Test MSE} will reach a minimum at some ``optimal'' level
      of model complexity.
      \vb
      \begin{itemize}
      \item Further complicating the model will increase Test MSE.
        \vb
      \item Note the blue line.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<"mse_fig", echo = FALSE, cache = TRUE>>=
maxPow <- 6
nObs   <- 100
nReps  <- 500

outList1 <- outList2 <- list()
for(rp in 1 : nReps) {
    x  <- rnorm(nObs)
    x2 <- rnorm(nObs)

    eta  <- x + x^2 + x^3 + x^4
    eta2 <- x2 + x2^2 + x2^3 + x2^4

    s2  <- (var(eta) / r2) - var(eta)
    s22 <- (var(eta2) / r2) - var(eta2)

    y  <- eta + rnorm(nObs, 0, sqrt(s2))
    y2 <- eta2 + rnorm(nObs, 0, sqrt(s22))

    form1 <- "y ~ 1"

    mse1 <- mse2 <- rep(NA, maxPow)
    for(p in 1 : maxPow) {
        form1 <- paste(form1, paste0("I(x^", p, ")"), sep = " + ")

        fit <- lm(as.formula(form1))

        yHat1 <- predict(fit)
        yHat2 <- predict(fit, newdata = data.frame(x = x2))

        mse1[p] <- crossprod(y - yHat1) / nObs
        mse2[p] <- crossprod(y2 - yHat2) / nObs
    }
    outList1[[rp]] <- mse1
    outList2[[rp]] <- mse2
}

mse1 <- colMeans(do.call(rbind, outList1))
mse2 <- colMeans(do.call(rbind, outList2))

p1 <- gg0(x = c(1 : maxPow), y = mse1, points = FALSE)
p1 + geom_point(colour = "red") +
    geom_line(colour = "red") +
    geom_point(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    geom_line(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    xlab("Polynomial Order") +
    ylab("MSE")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Training vs. Testing MSE}

  At the end of our model building example, we compared the following two
  models:
  \begin{align}
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{LDL} + \beta_3 X_{HDL} +
    \beta_4 X_{BMI} + \varepsilon \label{fullMod}\\
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{BMI} +
    \varepsilon \label{resMod}
  \end{align}
  \vx{-12}
  \begin{itemize}
  \item The $\Delta R^2$ test suggested that the loss in fit between Model
    \ref{fullMod} and Model \ref{resMod} was trivial.
    \vc
  \item The AIC and BIC both suggested that Model \ref{resMod} should be
    preferred over Model \ref{fullMod}.
    \vc
  \item The training MSE values suggested that Model \ref{fullMod} should be
    preferred.
  \end{itemize}
  \vb
  What happens when we do the comparison based on testing MSE instead of
  training MSE?

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Test MSE}

<<>>=
set.seed(235711)

## Read in the data:
dataDir <- "../../../data/"
dDat    <- readRDS(paste0(dataDir, "diabetes.rds"))

## Split data into training and testing sets:
ind  <- sample(1 : nrow(dDat))
dat0 <- dDat[ind[1 : 400], ]   # Training data
dat1 <- dDat[ind[401 : 442], ] # Testing data

## Fit the models:
outF <- lm(bp ~ age + bmi + ldl + hdl, data = dat0)
outR <- lm(bp ~ age + bmi, data = dat0)

## Compute training MSEs:
trainMseF <- MSE(y_pred = predict(outF), y_true = dat0$bp)
trainMseR <- MSE(y_pred = predict(outR), y_true = dat0$bp)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Test MSE}

<<>>=
## Compute testing MSEs:
testMseF <- MSE(y_pred = predict(outF, newdata = dat1),
                y_true = dat1$bp)
testMseR <- MSE(y_pred = predict(outR, newdata = dat1),
                y_true = dat1$bp)
@

Compare the two approaches:

<<echo = FALSE, results = "asis">>=
tab1 <- xtable(
    matrix(c(trainMseF, testMseF, trainMseR, testMseR),
           ncol = 2,
           dimnames = list(c("Train", "Test"), c("Full", "Restricted"))
           ),
    caption = "MSE Values")

print(tab1, digits = 2, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Cross-Validation}

%------------------------------------------------------------------------------%

\begin{frame}{Cross-Validation}

  To train a model that best predicts new data, we can use
  \emph{cross-validation} to evaluate the expected predictive performance on new
  data.
  \vc
  \begin{enumerate}
  \item Split the sample into two, disjoint sub-samples
    \begin{itemize}
    \item \emph{Training} data
    \item \emph{Testing} data
    \end{itemize}
    \vc
  \item Estimate a candidate model, $f(\mathbf{X})$, on the training data.
    \label{train}
    \vc
  \item Check the predictive performance of $\hat{f}(\mathbf{X})$ on the
    testing data. \label{test}
  \end{enumerate}
  \vb
  \pause
  We can use this idea to select the best model from a pool of candidate models,
  $\mathcal{F} = \left\{f_1(X), f_2(X), \ldots, f_J(X)\right\}$
  \vc
  \begin{enumerate}
  \item Repeat Steps \ref{train} and \ref{test} for all candidate models in
    $\mathcal{F}$.
    \vc
  \item Pick the $\hat{f}_j(\mathbf{X})$ that best predicts the testing data.
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Different Flavors of Cross-Validation}

  In practice, the split-sample cross-validation procedure describe above can be
  highly variable.
  \vc
  \begin{itemize}
  \item The solution is highly sensitive to the way the sample is split because
    each model is only training once.
  \end{itemize}
  \vb
  Split-sample cross-validation can also be wasteful.
  \vc
  \begin{itemize}
  \item We don't need to set aside an entire chunk of data for validation.
  \end{itemize}
  \vb
  In most cases, we will want to employ a slightly more complex flavor of
  cross-validation:
  \vc
  \begin{itemize}
  \item \emph{K-fold cross-validation}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{\emph{K}-Fold Cross-Validation}

  \begin{enumerate}
  \item Partition the data into $K$ disjoint subsets $C_k = C_1, C_2, \ldots,
    C_K$.
    \pause
    \vb
  \item Conduct $K$ training replications.
    \vb
    \begin{itemize}
    \item For each training replication, collapse $K - 1$ partitions into
      a set of training data, and use this training data to estimate the model.
      \vb
    \item Compute the test MSE for the $k$th partition, $MSE_k$, by using
      subset $C_k$ as the test data for the $k$th fitted model.
    \end{itemize}
    \pause
    \vb
  \item Compute the overall K-fold cross-validation error as:
  \end{enumerate}
  \begin{align*}
    CVE = \sum_{k = 1}^K \frac{N_k}{N}MSE_k,
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Applying \emph{K}-Fold CV to our Example}

  \begin{columns}
    \begin{column}{0.4\textwidth}

<<fig.show = "hide">>=
cvOutF <- CVlm(data = diabetes,
               form.lm = outF,
               m = 5,
               printit = FALSE,
               seed = 235711)

## Estimated CVE:
attr(cvOutF, "ms")
@

\end{column}
\begin{column}{0.6\textwidth}

<<echo = FALSE>>=
cvOutF <-
    CVlm(data = diabetes,
         form.lm = outF,
         m = 5,
         printit = FALSE,
         seed = 235711)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Applying \emph{K}-Fold CV to our Example}

  \begin{columns}
    \begin{column}{0.4\textwidth}

<<fig.show = "hide">>=
cvOutR <- CVlm(data = diabetes,
               form.lm = outR,
               m = 5,
               printit = FALSE,
               seed = 235711)

## Estimated CVE:
attr(cvOutR, "ms")
@

\end{column}
\begin{column}{0.6\textwidth}

<<echo = FALSE>>=
cvOutR <-
    CVlm(data = diabetes,
         form.lm = outR,
         m = 5,
         printit = FALSE,
         seed = 235711)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}

