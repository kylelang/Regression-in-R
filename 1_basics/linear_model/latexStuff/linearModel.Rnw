%%% Title:    Overview of Linear Regression
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-12
%%% Modified: 2021-11-09

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
%\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
%\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{relsize}

\title{Introduction to Linear Modeling}
\subtitle{Fundamental Techniques in Data Science with R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)

source("../../../code/supportFunctions.R")

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/linearModel-",
               message = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\section{The Regression Problem}

%------------------------------------------------------------------------------%

\begin{frame}{Regression Problem}

  Some of the most ubiquitous and useful statistical models are \emph{regression
    models}.
  \vb
  \begin{itemize}
  \item \emph{Regression} problems (as opposed to \emph{classification}
    problems) involve modeling a quantitative response.
    \vb
  \item The regression problem begins with a random outcome variable, $Y$.
    \vb
  \item We hypothesize that the mean of $Y$ is dependent on some set of
    fixed covariates, $\mathbf{X}$.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Flavors of Probability Distribution}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      The distributions with which you're probably most familiar imply a
      constant mean.
      \vb
      \begin{itemize}
      \item Each observation is expected to have the same value of $Y$,
        regardless of their individual characteristics.
        \vb
      \item This type of distribution is called ``marginal'' or ``unconditional.''
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
x <- seq(-4.0, 4.0, 0.001)

dat <- data.frame(X = x, density = dnorm(x))

p <- ggplot(dat, aes(x = X, y = density)) + theme_classic() +
    coord_cartesian(xlim = c(-4, 4)) +
    theme(text = element_text(size = 16, family = "Courier"))

p + geom_line() +
    geom_vline(xintercept = 0, linetype = "dotted", size = 1)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Flavors of Probability Distribution}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      The distributions we consider in regression problems have
      \emph{conditional means}.
      \vb
      \begin{itemize}
      \item The value of $Y$ that we expect for each observation is defined by
        the observations' individual characteristics.
        \vb
      \item This type of distribution is called ``conditional.''
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width = \textwidth]{%
          figures/conditional_density_figure.png%
        }\\
        \va
        \tiny{Image retrieved from:
            \url{http://www.seaturtle.org/mtn/archives/mtn122/mtn122p1.shtml}}
      \end{figure}

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Flavors of Probability Distribution}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      Even a simple comparison of means implies a conditional distribution.
      \vb
      \begin{itemize}
      \item The solid curve corresponds to outcome values for one group.
        \vb
      \item The dashed curve represents outcomes from the other group.
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}


<<echo = FALSE, cache = TRUE>>=
x   <- seq(0, 13, 0.001)
dat <- data.frame(x  = x,
                  yA = dnorm(x, 7, 1.5),
                  yB = dnorm(x, 5, 1.0)
                  )

p <- ggplot(data = dat) + coord_cartesian(xlim = c(0, 13)) + theme_classic() +
    geom_line(mapping = aes(x = x, y = yA)) +
    geom_line(mapping = aes(x = x, y = yB), linetype = "dashed") +
    labs(title = "Conditional distribution of outcomes",
         y     = "Density",
         x     = "Outcome") +
    theme(text = element_text(size = 16, family = "Courier"),
          plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
          )
p
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Projecting a Distribution onto the Plane}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      In practice, we only interact with the X-Y plane of the previous 3D
      figure.
      \vb
      \begin{itemize}
      \item On the Y-axis, we plot our outcome variable
        \vb
      \item The X-axis represents the predictor variable upon which we condition
        the mean of $Y$.
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
data(Cars93)

out1 <- lm(Horsepower ~ Price, data = Cars93)
Cars93$yHat  <- fitted(out1)
Cars93$yMean <- mean(Cars93$Horsepower)

p <- ggplot(data = Cars93, aes(x = Price, y = Horsepower)) +
    coord_cartesian() +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))
p1 <- p + geom_point()
p1
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Modeling the X-Y Relationship in the Plane}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      We want to explain the relationship between $Y$ and $X$ by finding the
      line that traverses the scatterplot as ``closely'' as possible to each
      point.
      \vb
      \begin{itemize}
      \item This is the ``best fit line''.
        \vb
      \item For any given value of $X$ the corresponding point on the best fit
        line is our best guess for the value of $Y$, given the model.
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
p1 + geom_smooth(method = 'lm', color = "blue", se = FALSE)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Simple Linear Regression}

%------------------------------------------------------------------------------%

\begin{frame}{Simple Linear Regression}

  The best fit line is defined by a simple equation:
  \begin{align*}
    \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X
  \end{align*}
  The above should look very familiar:
  \begin{align*}
    Y &= m X + b\\
      &= \hat{\beta}_1 X + \hat{\beta}_0
  \end{align*}
  $\hat{\beta}_0$ is the \emph{intercept}.
  \begin{itemize}
  \item The $\hat{Y}$ value when $X = 0$.
  \item The expected value of $Y$ when $X = 0$.
  \end{itemize}
  \vb
  $\hat{\beta}_1$ is the \emph{slope}.
  \begin{itemize}
  \item The change in $\hat{Y}$ for a unit change in $X$.
  \item The expected change in $Y$ for a unit change in $X$.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Thinking about Error}

  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      The equation $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$ only describes
      the best fit line.
      \begin{itemize}
      \item It does not fully quantify the relationship between $Y$ and $X$.\\
      \end{itemize}
      \vb
      \only<2>{
        We still need to account for the estimation error.
        \begin{align*}
          Y = {\color{blue}\hat{\beta}_0 + \hat{\beta}_1 X} + {\color{red}\hat{\varepsilon}}
        \end{align*}
      }
    \end{column}

    \begin{column}{0.5\textwidth}

      \only<1>{
<<echo = FALSE, cache = TRUE>>=
p1 + geom_smooth(method = 'lm', color = "blue", se = FALSE)
@
      }
      \only<2>{

<<echo = FALSE, cache = TRUE>>=
p2 <- p + geom_smooth(method = "lm", color = "blue", se = FALSE) +
    geom_segment(aes(x = Price, y = Horsepower, xend = Price, yend = yHat),
                 color = "red") +
    geom_point()
p2
@
      }

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Estimating the Regression Coefficients}

  The purpose of regression analysis is to use a sample of $N$ observed $\{Y_n,
  X_n\}$ pairs to find the best fit line defined by $\hat{\beta}_0$ and
  $\hat{\beta}_1$.
  \vb
  \begin{itemize}
  \item The most popular method of finding the best fit line involves minimizing
    the sum of the squared residuals.
    \vb
  \item $RSS = \sum_{n = 1}^N \hat{\varepsilon}_n^2$
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Residuals as the Basis of Estimation}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      The $\hat{\varepsilon}_n$ are defined in terms of deviations between each
      observed $Y_n$ value and the corresponding $\hat{Y}_n$.
      \begin{align*}
        \hat{\varepsilon}_n = Y_n - \hat{Y}_n =
        Y_n - \left(\hat{\beta}_0 + \hat{\beta}_1 X_n\right)
      \end{align*}
      Each $\hat{\varepsilon}_n$ is squared before summing to remove negative
      values.
      \begin{align*}
        RSS &= \sum_{n = 1}^N \hat{\varepsilon}_n^2 =
              \sum_{n = 1}^N \left(Y_n - \hat{Y}_n\right)^2\\
            &= \sum_{n = 1}^N \left(Y_n - \hat{\beta}_0 - \hat{\beta}_1
              X_n\right)^2
      \end{align*}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
p + geom_smooth(method = "lm", color = "blue", se = FALSE) +
    geom_segment(aes(x = Price, y = Horsepower, xend = Price, yend = yHat),
                 color = "red") +
    geom_point()
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Least Squares Example}

  Estimate the least squares coefficients for our example data:

<<cache = TRUE>>=
#data(Cars93)
out1 <- lm(Horsepower ~ Price, data = Cars93)
coef(out1)
@

<<echo = FALSE, cache = TRUE>>=
b0 <- round(coef(out1)[1], 2)
b1 <- round(coef(out1)[2], 2)
@

The estimated intercept is $\hat{\beta}_0 = \Sexpr{b0}$.
\begin{itemize}
\item A free car is expected to have \Sexpr{b0} horsepower.
\end{itemize}
\vb
The estimated slope is: $\hat{\beta}_1 = \Sexpr{b1}$.
\begin{itemize}
\item For every additional \$1000 in price, a car is expected to gain \Sexpr{b1}
  horsepower.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Inference for Regression Parameters}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Sampling Distribution}

  Sampling distribution = Probability distribution of a parameter.
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}

      \begin{itemize}
      \item The \emph{population} is defined by an infinite sequence of repeated
        estimations.
        \vb
        \begin{itemize}
        \item The sampling distribution quantifies the possible values of the
          statistic over infinite repeated sampling.
        \end{itemize}
        \vb
      \item The area of a region under the curve represents the probability of
        observing a \emph{statistic} within the corresponding interval.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
b  <- coef(out1)[2]
se <- sqrt(diag(vcov(out1)))[2]

x    <- seq((b - 4 * se), (b + 4 * se), length.out = 1000)
dat1 <- data.frame(B = x, density = dnorm(x = x, b, se))

p0 <- ggplot(dat1, aes(x = B, y = density)) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"),
          plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
          )
p0 + labs(title = "Sampling Distribution of Slope")
@

\end{column}
\end{columns}
\vb
\textbf{\emph{Intuition}}: \url{http://onlinestatbook.com/stat_sim/sampling_dist/}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Test Statistics}

  To ``test'' a slope coefficient, $\hat{\beta}$, we need a point of comparison.
  \begin{itemize}
  \item The \emph{null-hypothesized} value of the slope,
    $H_0: \beta = \tilde{\beta}$.
  \end{itemize}

  \vb

  Our hypothesis test is actually a test for the size of the difference:
  $\hat{\beta} - \tilde{\beta}$
  \begin{itemize}
  \item We define a \emph{test statistic}, $t$, to quantify the size of this
    difference accounting for the precision with which we've estimated
    $\hat{\beta}$.
  \end{itemize}

  \vb

  We can construct the test statistic for $\hat{\beta}$ as follows:
  \begin{align*}
    t = \frac{\hat{\beta} - \tilde{\beta}}{SE\left(\hat{\beta}\right)}
    ~~\overset{\tilde{\beta} = 0}{\mathlarger{\Rightarrow}}~~
    t = \frac{\hat{\beta} - 0}{SE\left(\hat{\beta}\right)} =
    \frac{\hat{\beta}}{SE\left(\hat{\beta}\right)}
  \end{align*}

<<echo = FALSE, cache = TRUE>>=
se1 <- round(se, 2)
df  <- out1$df.residual
moe <- round(qt(0.975, df = df) * se1, 2)
@

For the slope in our example, we get a test statistic of:
\begin{align*}
  t = \frac{\hat{\beta}_1}{SE\left(\hat{\beta}_1\right)} =
    \frac{\Sexpr{b1}}{\Sexpr{se1}} = \Sexpr{b1 / se1}
\end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Sampling Distribution of Test Statistic}

 \begin{columns}
   \begin{column}{0.5\textwidth}

     The t-statistic also has a sampling distribution.
     \vc
     \begin{itemize}
     \item Quantifies the possible values we could get if we repeatedly drew
       samples, of the \underline{same size}, from the \underline{same
         population} and re-computed a t-statistic each time.
     \vb
     \item The distribution under the null hypothesis assumes a population
       wherein $\hat{\beta}$ = $\tilde{\beta}$, and, consequently, $t = 0$.
     \end{itemize}

   \end{column}
   \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
x    <- seq(-4, 4, length.out = 1000)
dat1 <- data.frame(t = x, density = dt(x = x, df = df))

p1 <- ggplot(dat1, aes(x = t, y = density)) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"),
          plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
          )

p1 + labs(title = "Sampling Distribution\nCentral t-Statistic")
@

   \end{column}
 \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{P-Values}

  Once we compute our estimated test statistic, $\hat{t}$, we compare it to the
  appropriate null-hypothesized sampling distribution.
  \vb
  \begin{itemize}
    \item By calculating the area in the null distribution that exceeds our
      estimated test statistic, we can compute the probability of observing the
      given test statistic, or one more extreme, if the null hypothesis were
      true.
      \vb
      \begin{itemize}
      \item In other words, we can compute the probability of having sampled the
        data we observed, or more unusual data, from a population wherein there
        is no true difference between $\hat{\beta}$ and $\tilde{\beta}$.
      \end{itemize}
      \vb
    \item This value is the infamous \emph{p-value}.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{P-Values}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
tHat <- 2.15

p1 <- p1 +
    geom_ribbon(data = dat1[dat1$t >= tHat, ],
                mapping = aes(x = t, ymin = 0, ymax = density),
                fill = "gray")
p1 + labs(title = "One-Tailed Test")
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
p1 + geom_ribbon(data = dat1[dat1$t <= -tHat, ],
                       mapping = aes(x = t, ymin = 0, ymax = density),
                       fill = "gray") +
    labs(title = "Two-Tailed Test")
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpreting P-Values}

<<echo = FALSE>>=
pv <- pt(q          = tHat,
         df         = df,
         lower.tail = FALSE)
@

Consider the one-tailed test for our estimated test-statistic of $\hat{t} =
\Sexpr{round(tHat, 2)}$ that produces a p-value of $p = \Sexpr{round(pv, 3)}$.
\vc
\begin{itemize}
\item We \emph{\underline{cannot}} say that there is a $\Sexpr{round(pv, 3)}$
  probability that the true mean difference is greater than zero.
  \vc
\item We \emph{\underline{cannot}} say that there is a $\Sexpr{round(pv, 3)}$
  probability that the alternative hypothesis is true.
  \vc
\item We \emph{\underline{cannot}} say that there is a $\Sexpr{round(pv, 3)}$
  probability that the null hypothesis is false.
  \vc
\item We \emph{\underline{cannot}} say that there is a $\Sexpr{round(pv, 3)}$
  probability that the observed result is due to chance alone.
  \vc
\item We \emph{\underline{cannot}} say that there is a $\Sexpr{round(pv, 3)}$
  probability of replicating the observed effect in future studies.
\end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Interpreting P-Values}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      The p-value tells us $P(t \geq \hat{t}|H_0)$
      \begin{itemize}
      \item What we really want to know is $P(H_0|t \geq \hat{t})$.
      \end{itemize}
      \vb
      All that we \emph{\underline{can}} say is that there is a
      $\Sexpr{round(pv, 3)}$ probability of observing a test statistic
      at least as large as $\hat{t}$, if the null hypothesis is true.
      \vc
      \begin{itemize}
      \item Our test uses the same logic as \emph{proof by contradiction}.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Interpreting P-Values}

\begin{columns}
  \begin{column}{0.5\textwidth}

    Note that $P(t \geq \hat{t}|H_0) \neq P(t = \hat{t}|H_0)$
    \begin{itemize}
    \item We \emph{\underline{cannot}} say that there is a
      $\Sexpr{round(pv, 3)}$ probability of observing $\hat{t}$, if the null
      hypothesis is true.
    \end{itemize}
    \vb
    The probability of observing any individual point on a continuous
    distribution is exactly zero.
    \vc
    \begin{itemize}
    \item $P(t = \hat{t}|H_0) = 0$
    \end{itemize}

  \end{column}
  \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1
@

\end{column}
\end{columns}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------------------------------------------%

\begin{frame}{Confidence Intervals}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      A sampling distribution quantifies the possible values of the statistic.
      \vc
      \begin{itemize}
      \item We can use this distribution to estimate a \emph{plausible range}
        for the population parameter.
        \vc
        \begin{enumerate}
        \item<2-> Exclude the tails of the distribution.
          \vc
        \item<3-> The remaining values represent a good guess for plausible
          population values of the parameter.
        \end{enumerate}
        \vc
      \item<3-> This range is known as the \emph{confidence interval}.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

\only<1>{
<<echo = FALSE, cache = TRUE>>=
p0 + labs(title = "Sampling Distribution of the Slope")
@
}

\only<2>{
<<echo = FALSE, cache = TRUE>>=
b  <- coef(out1)[2]
se <- sqrt(diag(vcov(out1)))[2]

x    <- seq((b - 4 * se), (b + 4 * se), length.out = 1000)
dat1 <- data.frame(B = x, density = dnorm(x = x, b, se))

ci <- confint(out1)[2, ]

dat2 <- data.frame(x = ci, y = dnorm(ci, b, se))

p0 <- p0 +
    geom_segment(data = dat2, mapping = aes(x = x, y = 0, xend = x, yend = y),
                 linetype = 2,
                 size = 1) +
    labs(title = "Confidence Bounds for Slope")
p0
@
}

\only<3>{
<<echo = FALSE, cache = TRUE>>=
p0 + geom_ribbon(data = dat1[dat1$B > ci[1] & dat1$B < ci[2], ],
                       mapping = aes(x = B, ymin = 0, ymax = density),
                       fill = "gray") +
    labs(title = "Confidence Interval for Slope")
@
}
\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Confidence Intervals}

  We can construct confidence intervals by:
  \begin{align*}
    CI = \hat{\beta} \pm t_{crit} \times SE\left(\hat{\beta}\right)
  \end{align*}

  For our example slope, we get a 95\% CI of:
  \begin{align*}
    CI_{95} = \Sexpr{b1} \pm \Sexpr{round(qt(0.975, df = df), 2)} \times
    \Sexpr{se1} = [\Sexpr{b1 - moe}; \Sexpr{b1 + moe}]
  \end{align*}
  Which suggests that we can be 95\% certain that the true value of $\beta_1$ is
  somewhere between $\Sexpr{b1 - moe}$ and $\Sexpr{b1 + moe}$.
  \vc
  \begin{itemize}
  \item We are \emph{95\% certain} in the sense that if we repeat this analysis
    an infinite number of times, 95\% of the CIs that we calculate will surround
    the true value of $\beta_1$.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpreting Confidence Intervals}

  Say we estimate a regression slope of $\hat{\beta}_1 = 0.5$ with an associated
  95\% confidence interval of $CI = [0.25; 0.75]$.
  \pause
  \vc
  \begin{itemize}
  \item We \emph{\underline{cannot}} say that there is 95\% chance that the true
    value of $\beta_1$ is between 0.25 and 0.75.
    \vc
  \item We \emph{\underline{cannot}} say that the true value of $\beta_1$ is
    between 0.25 and 0.75, with probability 0.95.
  \end{itemize}
  \vb
  \pause
  The true value of $\beta_1$ is fixed; it's a single quantity.
  \begin{itemize}
  \item $\beta_1$ is either in our estimated interval or it is not; there is no
    uncertainty.
    \vc
  \item The probability that $\beta_1$ is within our estimated interval is either
    exactly 1 or exactly 0.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Interpreting Confidence Intervals}

  We don't talk about 95\% probabilities when interpreting CIs; instead, we
  talk about 95\% confidence.
  \vc
  \begin{columns}
    \begin{column}{0.5\textwidth}

      \begin{itemize}
      \item If we collected a new sample---of the same size---re-estimated our
        model, and re-computed the 95\% CI for $\hat{\beta_1}$, we would get a
        different interval.
        \vc
      \item Repeating this process an infinite number of times would give us a
        distribution of CIs.
        \vc
      \item 95\% of those CIs would surround the true value of $\beta_1$.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
reps <- 100
n    <- 500

ci <- beta <- list()
for(rp in 1 : reps) {
    x <- rnorm(n)
    y <- 0.5 * x + rnorm(n)

    fit <- lm(y ~ x)

    beta[[rp]] <- coef(fit)[2]
    ci[[rp]]   <- confint(fit)[2, ]
}

ci   <- do.call(rbind, ci)
beta <- unlist(beta)

check <- ci[ , 1] <= 0.5 & ci[ , 2] >= 0.5

cols         <- rep("blue", length(check))
cols[!check] <- "red"

dat1 <- data.frame(ind = 1 : reps, beta = beta, lb = ci[ , 1], ub = ci[ , 2])

ggplot(dat1, mapping = aes(x = ind, y = beta, group = 1)) +
    geom_point(colour = cols) +
    geom_errorbar(width   = 0.1,
                  mapping = aes(ymin = lb, ymax = ub),
                  colour  = cols) +
    geom_abline(slope = 0.0, intercept = 0.5) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) +
    ylab("Beta") +
    xlab("Replication") +
    labs(title = "100 95% CIs for Beta = 0.5") +
    theme(plot.title = element_text(size = 20, face = "bold", hjust = 0.5))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inference with Confidence Intervals}

  In terms of sampling distributions, our inferential task is to say something
  about how distinct the null and alternative distributions are.

<<echo = FALSE>>=
x0 <- seq(-4, 4, length.out = 1000)
x1 <- seq(-1.5, 6.5, length.out = 1000)
x2 <- seq(-2.5, 5.5, length.out = 1000)

h0 <- dnorm(x0)
h1 <- dnorm(x1, mean = 2.5)
h2 <- dnorm(x2, mean = 1.5)

ci1 <- qnorm(c(0.025, 0.975), mean = 2.5)
ci2 <- qnorm(c(0.025, 0.975), mean = 1.5)

dat2 <- data.frame(x0, x1, x2, h0, h1, h2)

ym0 <- dat2$h0[which.max(dat2$h0)]
ym1 <- dat2$h1[which.max(dat2$h1)]
ym2 <- dat2$h2[which.max(dat2$h2)]

yl1 <- dat2$h1[which.min(abs(dat2$x1 - ci1[1]))]
yu1 <- dat2$h1[which.min(abs(dat2$x1 - ci1[2]))]
yl2 <- dat2$h2[which.min(abs(dat2$x2 - ci2[1]))]
yu2 <- dat2$h2[which.min(abs(dat2$x2 - ci2[2]))]

p0 <- ggplot(dat2, aes(x = x0, y = h0)) +
    geom_line() +
    geom_segment(mapping  = aes(x = 0, y = 0, xend = 0, yend = ym0),
                 linetype = 2) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) +
    xlab("Beta") +
    ylab("Density")
@

\begin{columns}
  \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1.1 <- p0 +
    geom_line(mapping = aes(x = x1, y = h1), colour = "blue") +
    geom_segment(mapping  = aes(x = 2.5, y = 0, xend = 2.5, yend = ym1),
                 colour   = "blue",
                 linetype = 2)
p1.1
@

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1.2 <- p0 +
    geom_line(mapping = aes(x = x2, y = h2), colour = "blue") +
    geom_segment(mapping  = aes(x = 1.5, y = 0, xend = 1.5, yend = ym2),
                 colour   = "blue",
                 linetype = 2)
p1.2
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Inference with Confidence Intervals}

  CIs give us a plausible range for the population value of $\beta$, so we can
  use CIs to support inference.

\begin{columns}
  \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1.1 + geom_segment(mapping  = aes(x = ci1[1], y = 0, xend = ci1[1], yend = yl1),
                    colour   = "red",
                    linetype = 1,
                    size     = 0.75) +
    geom_segment(mapping  = aes(x = ci1[2], y = 0, xend = ci1[2], yend = yu1),
                 colour   = "red",
                 linetype = 1,
                 size     = 0.75)
@

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1.2 + geom_segment(mapping  = aes(x = ci2[1], y = 0, xend = ci2[1], yend = yl2),
                    colour   = "red",
                    linetype = 1,
                    size     = 0.75) +
    geom_segment(mapping  = aes(x = ci2[2], y = 0, xend = ci2[2], yend = yu2),
                 colour   = "red",
                 linetype = 1,
                 size     = 0.75)
@

\end{column}
\end{columns}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Model-Based Prediction}

  In the social and behavioral sciences, regression modeling is often focused on
  inference about estimated model parameters.
  \vc
  \begin{itemize}
  \item The association between the price of a car and its power.
    \vc
  \item We model the system and scrutinize $\hat{\beta}_1$ to make inferences
    about the association between price and power.
  \end{itemize}
  \vb
  \pause
  In data science applications, we're often more interested in predicting the
  outcome for new observations.
  \vc
  \begin{itemize}
  \item After we estimate $\hat{\beta}_0$ and $\hat{\beta}_1$, we can plug in
    new predictor data and get a predicted outcome value for any new case.
  \vc
  \item In our example, these predictions represent the projected horsepower
    ratings of cars with prices given by the new $X_{price}$ values.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Inference vs. Prediction}

  When doing statistical inference, we focus on how certain variables relate to
  the outcome.
  \begin{itemize}
  \item Do men have higher job-satisfaction than women?
  \item Does increased spending on advertising correlate with more sales?
  \item Is there a relationship between the number of liquor stores in a
    neighborhood and the amount of crime?
  \end{itemize}

  \vb
  \pause

  When doing prediction (or classification), we want to build a tool that can
  accurately guess future values.
  \begin{itemize}
  \item Will it rain tomorrow?
  \item How much will a company earn from investing in a certain research
    profile?
  \item What is a patient's risk of heart disease based on their medical history
    and test results?
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Model Fit}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Model Fit}
  We may also want to know how well our model explains the outcome.
  \begin{itemize}
  \item Our model explains some proportion of the outcome's variability.
  \item The residual variance $\hat{\sigma}^2 = \textrm{Var}(\hat{\varepsilon})$
    will be less than $\textrm{Var}(Y)$.
  \end{itemize}

  \begin{columns}
    \begin{column}{0.4\textwidth}
      \only<1>{
<<echo = FALSE>>=
dat3 <- data.frame(y = Cars93$Horsepower, r = resid(out1))

p10 <- ggplot(data = dat3, aes(x = y)) +
    coord_cartesian() + theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))

p10 + geom_density() + xlim(0, 350) + labs(x = "Original Outcome")
@
      }
      \only<2>{
<<echo = FALSE>>=
p5 <- ggplot(data = Cars93, aes(x = Price, y = Horsepower)) +
    coord_cartesian() +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))

p5 + geom_smooth(method = "lm", formula = y ~ 1, color = "blue", se = FALSE) +
    geom_segment(aes(x = Price, y = Horsepower, xend = Price, yend = yMean),
                 color = "red") +
    geom_point()
@
      }
    \end{column}
    \begin{column}{0.1\textwidth}

      \Huge{$\rightarrow$}

    \end{column}
    \begin{column}{0.4\textwidth}

      \only<1>{
<<echo = FALSE>>=
p11 <- ggplot(data = dat3, aes(x = r)) +
    coord_cartesian() + theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))

p11 + geom_density() + xlim(-140, 150) + labs(x = "Residuals")
@
      }
      \only<2>{

<<echo = FALSE>>=
p7 <- p5 + geom_smooth(method = "lm", color = "blue", se = FALSE) +
    geom_segment(aes(x = Price, y = Horsepower, xend = Price, yend = yHat),
                 color = "red") +
    geom_point()
p7
@
      }

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[shrink = 5]{Model Fit}

  We quantify the proportion of the outcome's variance that is explained by our
  model using the $R^2$ statistic:
  \begin{align*}
    R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
  \end{align*}
  where
  \begin{align*}
    TSS = \sum_{n = 1}^N \left(Y_n - \bar{Y}\right)^2 =
    \textrm{Var}(Y)\times (N - 1)
  \end{align*}

<<echo = FALSE, cache = TRUE>>=
ssr <- round(crossprod(resid(out1)))
sst <- round(crossprod(scale(Cars93$Horsepower, scale = FALSE)))
r2 <- round(1 - (ssr / sst), 2)
@

For our example problem, we get:
\begin{align*}
  R^2 = 1 - \frac{\Sexpr{as.integer(ssr)}}{\Sexpr{as.integer(sst)}} \approx
  \Sexpr{r2}
\end{align*}
Indicating that car price explains \Sexpr{r2 * 100}\% of the variability in
horsepower.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Model Fit for Prediction}

  When assessing predictive performance, we will most often use the \emph{mean
    squared error} (MSE) as our criterion.
  \vb
  \begin{align*}
    MSE &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{Y}_n\right)^2\\
    &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{\beta}_0 -
    \sum_{p = 1}^P \hat{\beta}_p X_{np} \right)^2\\
    &= \frac{RSS}{N}
  \end{align*}

<<echo = FALSE, cache = TRUE>>=
mse <- round(ssr / nrow(Cars93), 2)
@

For our example problem, we get:
\begin{align*}
  MSE = \frac{\Sexpr{as.integer(ssr)}}{\Sexpr{nrow(Cars93)}} \approx \Sexpr{mse}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpreting MSE}

<<echo = FALSE>>=
rmse <- round(sqrt(ssr/nrow(Cars93)), 2)
@

  The MSE quantifies the average squared prediction error.
  \begin{itemize}
  \item Taking the square root improves interpretation.
  \end{itemize}
  \begin{align*}
    RMSE = \sqrt{MSE}
  \end{align*}
  The RMSE estimates the magnitude of the expected prediction error.
  \begin{itemize}
  \item For our example problem, we get:
  \end{itemize}
  \begin{align*}
    RMSE = \sqrt{\frac{\Sexpr{as.integer(ssr)}}{\Sexpr{nrow(Cars93)}}} \approx
    \Sexpr{rmse}
  \end{align*}
  \vx{-8}
  \begin{itemize}
  \item When using price as the only predictor of horsepower, we expect
    prediction errors with magnitudes of \Sexpr{rmse} horsepower.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Information Criteria}

  We can use \emph{information criteria} to quickly compare \emph{non-nested}
  models while accounting for model complexity.\\

  \vb
  \begin{itemize}
  \item Akaike's Information Criterion (AIC)
    \only<1>{
      \begin{align*}
        AIC = 2K - 2\hat{\ell}(\theta|X)
      \end{align*}
    }
    \only<2>{
      \begin{align*}
        AIC = {\color{red}2K} - 2 {\color{blue}\hat{\ell}(\theta|X)}
      \end{align*}
    }
  \item Bayesian Information Criterion (BIC)
    \only<1>{
      \begin{align*}
        BIC = K\ln(N) - 2\hat{\ell}(\theta|X)
      \end{align*}
    }
    \only<2>{
      \begin{align*}
        BIC = {\color{red}K\ln(N)} - 2 {\color{blue}\hat{\ell}(\theta|X)}
      \end{align*}
    }
  \end{itemize}
  \onslide<2>{
    Information criteria balance two competing forces.
    \vc
    \begin{itemize}
    \item \blue{The optimized loglikelihood quantifies fit to the data.}
      \vc
    \item \red{The penalty term corrects for model complexity}.
    \end{itemize}
  }

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Information Criteria}

<<include = FALSE>>=
ll1 <- logLik(out1)

6 - 2 * ll1
AIC(out1)

k <- 3
n <- nrow(Cars93)
@

  For our example, we get the following estimates of AIC and BIC:
  \begin{align*}
    AIC &= 2(\Sexpr{k}) - 2(\Sexpr{round(ll1, 2)})\\
    &= \Sexpr{round(AIC(out1), 2)}\\[8pt]
    BIC &= \Sexpr{k}\ln(\Sexpr{n}) - 2(\Sexpr{round(ll1, 2)})\\
    &= \Sexpr{round(BIC(out1), 2)}
  \end{align*}

  To compute the AIC/BIC from a fitted \texttt{lm()} object in R:
<<>>=
AIC(out1)
BIC(out1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Multiple Linear Regression}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Graphical Representations}

  A regression of two variables can be represented on a 2D scatterplot.
  \begin{itemize}
  \item Simple linear regression implies a 1D line in 2D space.
  \end{itemize}
  \vb
  \begin{columns}
    \begin{column}{0.45\textwidth}

<<echo = FALSE, cache = TRUE>>=
p <- ggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) +
    labs(x = "Weight", y = "MPG") +
    geom_point(aes(size = 3), show.legend = FALSE, colour = "blue")
p
@

\end{column}

\begin{column}{0.1\textwidth}

  \begin{center}\huge{$\rightarrow$}\end{center}

\end{column}

\begin{column}{0.45\textwidth}

<<echo = FALSE, cache = TRUE>>=
p + geom_smooth(method = "lm", se = FALSE, col = "black")
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Graphical Representations}

  Adding an additional predictor leads to a 3D point cloud.
  \vb
  \begin{itemize}
  \item A regression model with two IVs implies a 2D plane in 3D space.
  \end{itemize}

  \begin{columns}
    \begin{column}{0.45\textwidth}

      \includegraphics[width = 1.2\textwidth]{figures/3d_data_plot}

    \end{column}

    \begin{column}{0.1\textwidth}

      \begin{center}\huge{$~~~~\rightarrow$}\end{center}

    \end{column}

    \begin{column}{0.45\textwidth}

      \includegraphics[width = 1.2\textwidth]{figures/response_surface_plot0}

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Partial Effects}

  In MLR, we want to examine the \emph{partial effects} of the predictors.
  \vb
  \begin{itemize}
  \item What is the effect of a predictor after controlling for some other set
    of variables?
  \end{itemize}
  \va
  This approach is crucial to controlling confounds and adequately modeling
  real-world phenomena.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<cache = TRUE>>=
## Read in the 'diabetes' dataset:
dDat <- readRDS("../data/diabetes.rds")

## Simple regression with which we're familiar:
out1 <- lm(bp ~ age, data = dDat)
@

\va

\textsc{Asking}: What is the effect of age on average blood pressure?

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Example}

<<cache = TRUE>>=
partSummary(out1, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<cache = TRUE>>=
## Add in another predictor:
out2 <- lm(bp ~ age + bmi, data = dDat)
@

\va

\textsc{Asking}: What is the effect of BMI on average blood pressure,
\emph{after controlling for age?}
\vb
\begin{itemize}
  \item We're partialing age out of the effect of BMI on blood pressure.
\end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Example}

<<cache = TRUE>>=
partSummary(out2, -1)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Interpretation}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      \begin{itemize}
      \item The expected average blood pressure for an unborn patient with a
        negligible extent is \Sexpr{round(coef(out2)[1], 2)}.
        \vb
      \item For each year older, average blood pressure is expected to increase
        by \Sexpr{round(coef(out2)['age'], 2)} points, after controlling for
        BMI.
        \vb
      \item For each additional point of BMI, average blood pressure is
        expected to increase by \Sexpr{round(coef(out2)['bmi'], 2)} points,
        after controlling for age.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

      \includegraphics[width = 1.1\textwidth]{figures/response_surface_plot2}

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Model Comparison}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Multiple $R^2$}

  How much variation in blood pressure is explained by the two models?
  \begin{itemize}
    \item Check the $R^2$ values.
  \end{itemize}

<<cache = TRUE>>=
## Extract R^2 values:
r2.1 <- summary(out1)$r.squared
r2.2 <- summary(out2)$r.squared

r2.1
r2.2
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{F-Statistic}

  How do we know if the $R^2$ values are significantly greater than zero?
  \begin{itemize}
  \item We use the F-statistic to test $H_0: R^2 = 0$ vs. $H_1: R^2 > 0$.
  \end{itemize}

<<>>=
f1 <- summary(out1)$fstatistic
f1
pf(q = f1[1], df1 = f1[2], df2 = f1[3], lower.tail = FALSE)
@

\pagebreak

<<>>=
f2 <- summary(out2)$fstatistic
f2
pf(f2[1], f2[2], f2[3], lower.tail = FALSE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Comparing Models}

  How do we quantify the additional variation explained by BMI, above and beyond
  age?
  \begin{itemize}
  \item Compute the $\Delta R^2$
  \end{itemize}

<<cache = TRUE>>=
## Compute change in R^2:
r2.2 - r2.1
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing}

How do we know if $\Delta R^2$ represents a significantly greater degree of
explained variation?
\begin{itemize}
\item Use an $F$-test for $H_0: \Delta R^2 = 0$ vs. $H_1: \Delta R^2 > 0$
\end{itemize}

<<>>=
## Is that increase significantly greater than zero?
anova(out1, out2)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Comparing Models}

  We can also compare models based on their prediction errors.
  \begin{itemize}
  \item For OLS regression, we usually compare MSE values.
  \end{itemize}
  \vx{-6}
<<>>=
mse1 <- MSE(y_pred = predict(out1), y_true = dDat$bp)
mse2 <- MSE(y_pred = predict(out2), y_true = dDat$bp)

mse1
mse2
@

In this case, the MSE for the model with $BMI$ included is smaller.
\begin{itemize}
\item We should prefer the the larger model.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Comparing Models}

  Finally, we can compare models based on information criteria.

<<>>=
AIC(out1, out2)
BIC(out1, out2)
@

In this case, both the AIC and the BIC for the model with $BMI$ included are
smaller.
\begin{itemize}
\item We should prefer the the larger model.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionslide{Categorical Predictors}

%------------------------------------------------------------------------------%

\begin{frame}{Categorical Predictors}

  Most of the predictors we've considered thus far have been
  \emph{quantitative}.
  \vc
  \begin{itemize}
  \item Continuous variables that can take any real value in their range
    \vc
  \item Interval or Ratio scaling
  \end{itemize}
  \vb
  We often want to include grouping factors as predictors.
  \vc
  \begin{itemize}
  \item These variables are \emph{qualitative}.
    \begin{itemize}
    \item Their values are simply labels.
      \vc
    \item There is no ordering of the categories.
      \vc
    \item Nominal scaling
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{How to Model Categorical Predictors}

  We need to be careful when we include categorical predictors into a regression
  model.
  \vc
  \begin{itemize}
  \item The variables need to be coded before entering the model
  \end{itemize}
  \vb
  Consider the following indicator of major:
  $X_{maj} = \{1 = \textit{Law}, 2 = \textit{Economics}, 3 = \textit{Data Science}\}$
  \vc
  \begin{itemize}
    \item What would happen if we na\"ively used this variable to predict
      program satisfaction?
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{How to Model Categorical Predictors}

<<>>=
mDat <- readRDS("../data/major_data.rds")
mDat[seq(25, 150, 25), ]
out1 <- lm(sat ~ majN, data = mDat)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{How to Model Categorical Predictors}

<<>>=
partSummary(out1, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{Dummy Coding}

%------------------------------------------------------------------------------%

\begin{frame}{Dummy Coding}

  The most common way to code categorical predictors is \emph{dummy coding}.
  \vb
  \begin{itemize}
  \item A $G$-level factor must be converted into a set of $G - 1$ dummy codes.
    \vb
  \item Each code is a variable on the dataset that equals 1 for observations
    corresponding to the code's group and equals 0, otherwise.
    \vb
  \item The group without a code is called the \emph{reference group}.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Dummy Code}
 Let's look at the simple example of coding biological sex:
<<echo = FALSE, results = "asis">>=
sex  <- factor(sample(c("male", "female"), 10, TRUE))
male <- as.numeric(model.matrix(~sex)[ , -1])

xTab2 <- xtable(data.frame(sex, male), digits = 0)
print(xTab2, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Dummy Codes}
 Now, a slightly more complex  example:
<<echo = FALSE, results = "asis">>=
drink <- factor(sample(c("coffee", "tea", "juice"), 10, TRUE))

codes           <- model.matrix(~drink)[ , -1]
colnames(codes) <- c("juice", "tea")

xTab3 <- xtable(data.frame(drink, codes), digits = 0)
print(xTab3, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Using Dummy Codes}

  To use the dummy codes, we simply include the $G - 1$ codes as $G - 1$
  predictor variables in our regression model.
  \begin{align*}
    Y &= \beta_0 + \beta_1 X_{male} + \varepsilon\\
    Y &= \beta_0 + \beta_1 X_{juice} + \beta_2 X_{tea} + \varepsilon
  \end{align*}
  \vx{-18}
  \begin{itemize}
  \item The intercept corresponds to the mean of $Y$ for the reference group.
    \vc
  \item Each slope represents the difference between the mean of $Y$ in the
    coded group and the mean of $Y$ in the reference group.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  First, an example with a single, binary dummy code:

<<>>=
## Read in some data:
cDat <- readRDS("../data/cars_data.rds")

## Fit and summarize the model:
out2 <- lm(price ~ mtOpt, data = cDat)
@

\pagebreak

<<>>=
partSummary(out2, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpretations}

  \begin{itemize}
  \item The average price of a car without the option for a manual transmission
    is $\hat{\beta}_0 = \Sexpr{round(coef(out2)[1], 2)}$ thousand dollars.
    \vb
  \item The average difference in price between cars that have manual
    transmissions as an option and those that do not is $\hat{\beta}_1 =
    \Sexpr{round(coef(out2)[2], 2)}$ thousand dollars.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

Fit a more complex model:

<<>>=
out3 <- lm(price ~ front + rear, data = cDat)
partSummary(out3, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpretations}

  \begin{itemize}
  \item The average price of a four-wheel-drive car is $\hat{\beta}_0 =
    \Sexpr{round(coef(out3)[1], 2)}$ thousand dollars.
    \vb
  \item The average difference in price between front-wheel-drive cars and
    four-wheel-drive cars is $\hat{\beta}_1 = \Sexpr{round(coef(out3)[2], 2)}$
    thousand dollars.
   \vb
  \item The average difference in price between rear-wheel-drive cars and
    four-wheel-drive cars is $\hat{\beta}_2 = \Sexpr{round(coef(out3)[3], 2)}$
    thousand dollars.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Include two sets of dummy codes:

<<>>=
out4 <- lm(price ~ mtOpt + front + rear, data = cDat)
partSummary(out4, -c(1, 2))
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpretations}

  \begin{itemize}
  \item The average price of a four-wheel-drive car that does not have a manual
    transmission option is $\hat{\beta}_0 = \Sexpr{round(coef(out4)[1], 2)}$
    thousand dollars.
    \vb
  \item After controlling for drive type, the average difference in price
    between cars that have manual transmissions as an option and those that do
    not is $\hat{\beta}_1 = \Sexpr{round(coef(out4)[2], 2)}$ thousand dollars.
    \vb
  \item After controlling for transmission options, the average difference in
    price between front-wheel-drive cars and four-wheel-drive cars is
    $\hat{\beta}_2 = \Sexpr{round(coef(out4)[3], 2)}$ thousand dollars.
   \vb
  \item After controlling for transmission options, the average difference in
    price between rear-wheel-drive cars and four-wheel-drive cars is
    $\hat{\beta}_3 = \Sexpr{round(coef(out4)[4], 2)}$ thousand dollars.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\subsection{Significance Testing for Dummy Codes}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing}

  For variables with only two levels, we can test the overall factor's
  significance by evaluating the significance of a single dummy code.

<<>>=
partSummary(out2, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing}

  For variables with more than two levels, we need to simultaneously evaluate
  the significance of each of the variable's dummy codes.

<<>>=
partSummary(out4, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing}

<<>>=
summary(out4)$r.squared - summary(out2)$r.squared
anova(out2, out4)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing}

  For models with a single nominal factor is the only predictor, we use the
  omnibus F-test.

<<>>=
partSummary(out3, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
