%%% Title:    Model-Building, Prediction, & Cross-Validation
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-13
%%% Modified: 2021-11-24

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{caption}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

%% Don't label tables:
\captionsetup[table]{labelformat = empty}

\title{Model-Building, Prediction, \& Cross-Validation}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(dplyr)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)
library(rockchalk)

source("../../../code/supportFunctions.R")

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/prediction-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}

  \tableofcontents

\end{frame}

%------------------------------------------------------------------------------%

\section{Model-Building}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Model-Building Example}

  Let's walk through an example of the model-building process.
  \vc
  \begin{itemize}
    \item We'll take $Y_{bp} = \beta_0 + \beta_1 X_{age.30} + \varepsilon$ as
      our baseline model.
      \vc
    \item Next, we simultaneously add predictors of LDL and HDL cholesterol.
  \end{itemize}

<<cache = TRUE>>=
diabetes <- readRDS("../data/diabetes.rds")

## Center predictor variables:
diabetes <- mutate(diabetes,
                   ldl100 = ldl - 100,
                   hdl60 = hdl - 60,
                   age30 = age - 30)

## Baseline model:
out1 <- lm(bp ~ age30, data = diabetes)

## Simultaneously add two predictors:
out2 <- lm(bp ~ age30 + ldl100 + hdl60, data = diabetes)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Model-Building Example}

<<cache = TRUE>>=
partSummary(out1, -1)
@

\pagebreak

<<cache = TRUE>>=
partSummary(out2, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpretations}

  \begin{itemize}
  \item The expected average blood pressure for a 30 year old patient with LDL =
    100 and HDL = 60 is \Sexpr{round(coef(out2)[1], 2)}.
    \vb
  \item For each additional year older, average blood pressure is expected to
    increase by \Sexpr{round(coef(out2)['age30'], 2)}, after controlling for LDL
    and HDL levels.
    \vb
  \item For each additional unit of LDL level, average blood pressure is
    expected to increase by \Sexpr{round(coef(out2)['ldl100'], 2)}, after
    controlling for age and HDL.
    \vb
  \item For each additional unit of HDL level, average blood pressure is
    expected to decrease by \Sexpr{round(coef(out2)['hdl60'], 2)}, after
    controlling for age and LDL.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

<<cache = TRUE>>=
## Compute change in R^2:
summary(out2)$r.squared - summary(out1)$r.squared

## Significance test for change in R^2:
anova(out1, out2)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

<<>>=
(mse1 <- MSE(y_pred = predict(out1), y_true = diabetes$bp))
(mse2 <- MSE(y_pred = predict(out2), y_true = diabetes$bp))

AIC(out1, out2)
BIC(out1, out2)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interpretations}

<<echo = FALSE>>=
r2.1 <- summary(out1)$r.squared
r2.2 <- summary(out2)$r.squared
@

\begin{itemize}
\item Age, LDL, and HDL explain a combined \Sexpr{round(100 * r2.2, 1)}\% of
  the variation in blood pressure.
  \vc
  \begin{itemize}
  \item This proportion of variation explained is significantly greater than
    zero.
  \end{itemize}
  \vb
\item Adding LDL and HDL produces a model that explains \Sexpr{round(100 *
  (r2.2 - r2.1), 1)}\% more variation in blood pressure than a model with age
  as the only predictor.
  \vc
  \begin{itemize}
  \item This increase in variation explained is significantly greater than zero.
  \end{itemize}
  \vb
\item Adding LDL and HDL produces a model with lower prediction error
  (i.e., MSE = \Sexpr{round(mse2, 2)} vs. MSE = \Sexpr{round(mse1, 2)}).
  \vb
\item Both the AIC and the BIC also suggest that adding LDL and HDL
  produces a better model.
\end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Continue Building the Model}

  So far we've established that age, LDL, and HDL are all significant predictors
  of average blood pressure.
  \vc
  \begin{itemize}
  \item We've also established that adding LDL and HDL, together, explain
    significantly more variation than age alone.
  \end{itemize}
  \vb
  Next, we'll add BMI to see what additional explanatory role it can play
  above and beyond age and cholesterol.

<<cache = TRUE>>=
## Center BMI:
diabetes <- mutate(diabetes, bmi25 = bmi - 25)

## Now, add bmi:
out3 <- lm(bp ~ age30 + ldl100 + hdl60 + bmi25, data = diabetes)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model-Building Example}

<<cache = TRUE>>=
partSummary(out3, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Interpretations}

  BMI seems to have a pretty strong effect on average blood pressure, after
  controlling for age and cholesterol levels.
  \vc
  \begin{itemize}
  \item After controlling for BMI, cholesterol levels no longer seem to be
    important predictors.
    \vc
  \item Let's take a look at what happens to the cholesterol effects when we add
    BMI:
  \end{itemize}

<<echo = FALSE, cache = TRUE, results = 'asis'>>=
tab1 <- rbind(coef(out2)[-c(1, 2)], coef(out3)[-c(1, 2, 5)])
rownames(tab1) <- c("Without BMI", "With BMI")
colnames(tab1) <- c("LDL", "HDL")
xTab1 <- xtable(tab1, align = rep("l", 3), digits = 3)
print(xTab1, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

  How much additional variability in blood pressure is explained by BMI above
  and beyond age and cholesterol levels?
<<cache = TRUE>>=
r2.3 <- summary(out3)$r.squared
r2.3 - r2.2
@

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

  Is the additional \Sexpr{round(100 * (r2.3 - r2.2), 2)}\% variation explained
  a significant increase?

<<>>=
anova(out2, out3)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

<<>>=
mse3 <- MSE(y_pred = predict(out3), y_true = diabetes$bp)

mse2
mse3

AIC(out2, out3)
BIC(out2, out3)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Model Modification}

  Maybe cholesterol levels are not important features once we've accounted for
  BMI.
  \vc
  \begin{itemize}
  \item Let's try a model including BMI but excluding cholesterol levels.
  \end{itemize}

<<cache = TRUE>>=
## Take out the cholesterol variables:
out4 <- lm(bp ~ age30 + bmi25, data = diabetes)
@

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Model-Building Example}

<<cache = TRUE>>=
partSummary(out4, -1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

  How much explained variation did we loose by removing the LDL and HDL
  variables?

<<cache = TRUE>>=
r2.4 <- summary(out4)$r.squared
r2.3 - r2.4
@

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

  Is this \Sexpr{round(100 * (r2.3 - r2.4), 2)}\% loss in explained variance
  significant?

<<>>=
anova(out4, out3)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Comparison}

<<>>=
mse4 <- MSE(y_pred = predict(out4), y_true = diabetes$bp)

mse3
mse4

AIC(out3, out4)
BIC(out3, out4)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Model-Based Prediction}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction}

  So far, we've focused mostly on inferences about the estimated regression
  coefficients.
  \vb
  \begin{itemize}
  \item Asking questions about how $X$ is related to $Y$.
  \end{itemize}
  \vb
  We can also use linear regression for \emph{prediction}.
  \vb
  \begin{itemize}
  \item Given a new observation, $X_m$, what outcome value, $\hat{Y}_m$, does
    our model attribute to the \emph{m}th observation?
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction}

  Train a model to predict psychological well-being from diet-related and
  exercise-related features.
  \vb
  \begin{itemize}
  \item Plug-in new feature values corresponding to an experimental wellness
    program to see the expected well-being for a hypothetical patient treated
    with the new program.
  \end{itemize}
  \vb
  Predict future gasoline prices based on geo-political events in oil-producing
  countries.
  \vb
  \begin{itemize}
    \item If conflict escalates in the Middle East, adjust the appropriate
      features and project likely changes in gasoline prices.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction Example}

  To fix ideas, let's reconsider the \emph{diabetes} data and the following
  model:
  \begin{align*}
    Y_{LDL} = \beta_0 + \beta_1 X_{BP} + \beta_2 X_{gluc} + \beta_3 X_{BMI} +
    \varepsilon
  \end{align*}

<<echo = FALSE>>=
trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp + glu + bmi, data = trainDat)
b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

confInt <- round(
    predict(out1, newdat = testDat, interval = "confidence")[1, 2 : 3], 2
)
predInt <- round(
    predict(out1, newdat = testDat, interval = "prediction")[1, 2 : 3], 2
)
@

Training this model on the first $N = 400$ patients' data produces the following
fitted model:
\begin{align*}
  \hat{Y}_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP} + \Sexpr{b2} X_{gluc} +
  \Sexpr{b3} X_{BMI}
\end{align*}
\pause
Suppose a new patient presents with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and
$BMI = \Sexpr{x3}$. We can predict their $LDL$ score by:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2}
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(predict(out1, testDat[1 : 2, ])[1], 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Interval Estimates for Prediction}

%------------------------------------------------------------------------------%

\begin{frame}{Interval Estimates for Prediction}

  To quantify uncertainty in our predictions, we want to use an appropriate
  interval estimate.
  \vb
  \begin{itemize}
  \item Two flavors of interval are applicable to predictions:
    \begin{enumerate}
    \item Confidence intervals for $\hat{Y}_m$
      \vc
    \item Prediction intervals for a specific observation, $Y_m$
    \end{enumerate}
    \vb
  \item The CI for $\hat{Y}_m$ gives a likely range (in the sense of coverage
    probability and ``confidence'') for the \emph{m}th value of the true conditional
    mean.
    \begin{itemize}
    \item CIs only account for uncertainty in the estimated regression
      coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$.
    \end{itemize}
    \vb
  \item The prediction interval for $Y_m$ gives a likely range (in the same
    sense as CIs) for the \emph{m}th outcome value.
    \begin{itemize}
    \item Prediction intervals also account for the regression errors,
      $\varepsilon$.
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Confidence vs. Prediction Intervals}

 \begin{columns}
   \begin{column}{0.5\textwidth}

     Let's visualize the predictions from a simple model:
     \begin{align*}
       Y_{BP} = {\color{blue} \hat{\beta}_0 + \hat{\beta}_1 X_{BMI}} +
       {\color{red} \hat{\varepsilon}}
     \end{align*}
     \vx{-12}
     \begin{itemize}
     \item<2-3> CIs for $\hat{Y}$ ignore the errors, ${\color{red}\varepsilon}$.
       \begin{itemize}
       \item They only care about the best-fit line,
         ${\color{blue} \beta_0 + \beta_1 X_{BMI}}$.
       \end{itemize}
       \vb
     \item<3> Prediction intervals are wider than CIs.
       \begin{itemize}
       \item They account for the additional uncertainty contributed by
         ${\color{red}\varepsilon}$.
       \end{itemize}
     \end{itemize}

   \end{column}
   \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
out1 <- lm(bp ~ bmi, data = trainDat)

testDat$preds <- predict(out1, newdata = testDat)

ci <- predict(out1, newdata = testDat, interval = "confidence")[ , -1]
pi <- predict(out1, newdata = testDat, interval = "prediction")[ , -1]

colnames(ci) <- c("ciLo", "ciHi")
colnames(pi) <- c("piLo", "piHi")

testDat <- data.frame(testDat, ci, pi)

p1 <- ggplot(data = testDat, aes(x = bmi, y = bp)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) +
    geom_segment(aes(x = bmi, y = bp, xend = bmi, yend = preds),
                 color = "red") +
    geom_line(mapping = aes(x = bmi, y = preds), color = "blue", size = 1) +
    geom_point() +
    ylim(range(pi))
@
\only<1>{
<<echo = FALSE, cache = TRUE>>=
print(p1)
@
}
\only<2>{
<<echo = FALSE, cache = TRUE>>=
p2 <- p1 +
    geom_line(mapping = aes(x = bmi, y = ciLo), size = 1, linetype = "solid") +
    geom_line(mapping = aes(x = bmi, y = ciHi), size = 1, linetype = "solid")
p2
@
}
\only<3>{
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
p2 +
    geom_line(mapping = aes(x = bmi, y = piLo), size = 1, linetype = "dashed") +
    geom_line(mapping = aes(x = bmi, y = piHi), size = 1, linetype = "dashed")
@
}

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Interval Estimates Example}

  Going back to our hypothetical ``new'' patient, we get the following $95\%$
  interval estimates:
  \begin{align*}
    95\% ~ CI_{\hat{Y}} &= [\Sexpr{confInt[1]}; \Sexpr{confInt[2]}]\\[6pt]
    95\% ~ PI &= [\Sexpr{predInt[1]}; \Sexpr{predInt[2]}]
  \end{align*}
  \vx{-16}
  \begin{itemize}
  \item We can be 95\% confident that the \underline{average \emph{LDL}} of
    patients with \emph{Glucose} = 89, \emph{BP} = 121, and \emph{BMI} = 30.6
    will be somewhere between \Sexpr{confInt[1]} and \Sexpr{confInt[2]}.
    \vb
  \item We can be 95\% confident that the \underline{\emph{LDL} of a specific
    patient} with \emph{Glucose} = 89, \emph{BP} = 121, and \emph{BMI} = 30.6
    will be somewhere between \Sexpr{predInt[1]} and \Sexpr{predInt[2]}.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Building \& Evaluating Predictive Models}

%------------------------------------------------------------------------------%

\begin{frame}{Specifying Predictive Models}

  When focused on inferences about regression coefficients, we care very much
  about the predictors entered into the model.
  \vb
  \begin{itemize}
  \item Partial regression coefficients must be interpreted as controlling for
    all other predictors.
  \end{itemize}
  \vb
  \pause
  When focused on prediction, we often don't care as much about the
  specific variables that enter the model.
  \vb
  \begin{itemize}
  \item We prefer whatever set of features produces the best predictive
    performance.
    \vb
  \item We may want to know which are the ``best'' predictors.
    \vc
    \begin{itemize}
    \item We usually want the data to ``give'' us this answer.
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Evaluating Predictive Performance}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      How do we assess ``good'' prediction?
      \vb
      \begin{itemize}
      \item Can we simply find the model that best predicts the data used to
        train the model?
        \vb
      \item What are we trying to do when building a predictive model?
        \vb
      \item Can we quantify this objective with some type of fit measure?
      \end{itemize}

      \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
nObs <- 10
r2   <- 0.9

x   <- rangeNorm(rnorm(nObs), 18, 60)
eta <- 0.035 * x + 0.015 * x^2
sig <- (var(eta) / r2) - var(eta)
y   <- eta + rnorm(nObs, 0, sqrt(sig))

x2   <- rangeNorm(rnorm(nObs), 18, 60)
eta2 <- 0.035 * x2 + 0.015 * x2^2
sig2 <- (var(eta2) / r2) - var(eta2)
y2   <- eta2 + rnorm(nObs, 0, sqrt(sig2))

form1 <- as.formula(
    paste0("y ~ x + ", paste0("I(x^", c(2 : 10), ")", collapse = " + "))
)

p1 <- gg0(x = x, y = y) + xlab("Age") + ylab("Mortality Risk")
p1
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Different Possible Models}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE)
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@

\end{column}
\end{columns}

  \va

\begin{columns}
  \begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2) + I(x^3))
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2) + I(x^3) + I(x^4))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Over-fitting}

%------------------------------------------------------------------------------%

\begin{frame}{Over-fitting}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We can easily go too far.
      \vb
      \begin{itemize}
      \item Enough polynomial terms will exactly replicate any data.
        \vb
      \item Is this what we're trying to do?
        \vb
      \item What kind of issues arise in the extreme case?
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = form1)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Over-fitting}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Should we be pleased to be able to perfectly predict mortality risk?
      \vb
      \begin{itemize}
      \item Is our model useful?
        \vc
      \item What happens if we try to apply our fitted model to new data?
      \end{itemize}

    \end{column}

    \pause

    \begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p2 <- gg0(x = x2, y = y2) + xlab("Age") + ylab("Mortality Risk")
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = form1)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Correct Fit}

  Let's try something a bit more reasonable.\\
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, warning = FALSE>>=
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2)
                 )
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{A Sensible Goal}

  Our goal is to train a model that can best predict \emph{new data}.
  \vb
  \begin{itemize}
  \item The predictive performance on the training data is immaterial.
    \vb
  \item We can always fit the training data arbitrarily well.
    \vb
  \item Fit to the training data will always be at-odds with fit to future data.
  \end{itemize}
  \vb
  This conflict the driving force behind the \emph{bias-variance trade-off}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Model Fit for Prediction}

  When assessing predictive performance, we will most often use the \emph{mean
    squared error} (MSE) as our criterion.
  \begin{align*}
    MSE &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{Y}_n\right)^2\\
    &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{\beta}_0 -
    \sum_{p = 1}^P \hat{\beta}_p X_{np} \right)^2\\
    &= \frac{RSS}{N}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Training vs. Testing Errors}

%------------------------------------------------------------------------------%

\begin{frame}{Training vs. Testing MSE}

  The MSE on the preceding slide is computing based entirely on training data.
  \vb
  \begin{itemize}
  \item \emph{Training MSE}
  \end{itemize}
  \vb
  What we want is a measure of fit to new, \emph{testing} data.
  \vb
  \begin{itemize}
  \item \emph{Testing MSE}
    \vb
  \item Given \emph{M} new observations $\{Y_m, X_{m1}, X_{m2}, \ldots, X_{mP}\}$,
    and a fitted regression model, $f(\mathbf{X})$, defined by the
    coefficients $\{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \ldots,
    \hat{\beta}_P\}$, the \emph{testing MSE} is given by:
    \begin{align*}
      MSE &= \frac{1}{M} \sum_{m = 1}^M \left(Y_m - \hat{\beta}_0 -
      \sum_{p = 1}^P \hat{\beta}_p X_{mp} \right)^2\\
    \end{align*}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Training vs. Testing MSE}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      \textcolor{red}{Training MSE} will always decrease in response to
      increased model complexity.
      \vb
      \begin{itemize}
      \item Note the red line in the plot
      \end{itemize}
      \vb
      \textcolor{blue}{Testing MSE} will reach a minimum at some
      ``optimal'' level of model complexity.
      \vb
      \begin{itemize}
      \item Further complicating the model will increase the testing MSE.
        \vb
      \item Note the blue line.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<"mse_fig", echo = FALSE, cache = TRUE>>=
maxPow <- 6
nObs   <- 100
nReps  <- 500

outList1 <- outList2 <- list()
for(rp in 1 : nReps) {
    x  <- rnorm(nObs)
    x2 <- rnorm(nObs)

    eta  <- x + x^2 + x^3 + x^4
    eta2 <- x2 + x2^2 + x2^3 + x2^4

    s2  <- (var(eta) / r2) - var(eta)
    s22 <- (var(eta2) / r2) - var(eta2)

    y  <- eta + rnorm(nObs, 0, sqrt(s2))
    y2 <- eta2 + rnorm(nObs, 0, sqrt(s22))

    form1 <- "y ~ 1"

    mse1 <- mse2 <- rep(NA, maxPow)
    for(p in 1 : maxPow) {
        form1 <- paste(form1, paste0("I(x^", p, ")"), sep = " + ")

        fit <- lm(as.formula(form1))

        yHat1 <- predict(fit)
        yHat2 <- predict(fit, newdata = data.frame(x = x2))

        mse1[p] <- crossprod(y - yHat1) / nObs
        mse2[p] <- crossprod(y2 - yHat2) / nObs
    }
    outList1[[rp]] <- mse1
    outList2[[rp]] <- mse2
}

mse1 <- colMeans(do.call(rbind, outList1))
mse2 <- colMeans(do.call(rbind, outList2))

p1 <- gg0(x = c(1 : maxPow), y = mse1, points = FALSE)
p1 + geom_point(colour = "red") +
    geom_line(colour = "red") +
    geom_point(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    geom_line(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    xlab("Polynomial Order") +
    ylab("MSE")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Training vs. Testing MSE}

  At the end of our model building example, we compared the following two
  models:
  \begin{align}
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{LDL} + \beta_3 X_{HDL} +
    \beta_4 X_{BMI} + \varepsilon \label{fullMod}\\
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{BMI} +
    \varepsilon \label{resMod}
  \end{align}
  \vx{-12}
  \begin{itemize}
  \item The $\Delta R^2$ test suggested that the loss in fit between Model
    \ref{fullMod} and Model \ref{resMod} was trivial.
    \vc
  \item The AIC and BIC both suggested that Model \ref{resMod} should be
    preferred over Model \ref{fullMod}.
    \vc
  \item The training MSE values suggested that Model \ref{fullMod} should be
    preferred.
  \end{itemize}
  \vb
  What happens when we do the comparison based on testing MSE instead of
  training MSE?

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Testing MSE}

<<>>=
set.seed(235711)

## Split data into training and testing sets:
ind  <- sample(1 : nrow(diabetes))
dat0 <- diabetes[ind[1 : 400], ]   # Training data
dat1 <- diabetes[ind[401 : 442], ] # Testing data

## Fit the models:
outF <- lm(bp ~ age + bmi + ldl + hdl, data = dat0)
outR <- lm(bp ~ age + bmi, data = dat0)

## Compute training MSEs:
trainMseF <- MSE(y_pred = predict(outF), y_true = dat0$bp)
trainMseR <- MSE(y_pred = predict(outR), y_true = dat0$bp)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Testing MSE}

<<>>=
## Compute testing MSEs:
testMseF <- MSE(y_pred = predict(outF, newdata = dat1),
                y_true = dat1$bp)
testMseR <- MSE(y_pred = predict(outR, newdata = dat1),
                y_true = dat1$bp)
@

Compare the two approaches:

<<echo = FALSE, results = "asis">>=
tab1 <- xtable(
    matrix(c(trainMseF, testMseF, trainMseR, testMseR),
           ncol = 2,
           dimnames = list(c("Train", "Test"), c("Full", "Restricted"))
           ),
    caption = "MSE Values")

print(tab1, digits = 2, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Cross Validation}

%------------------------------------------------------------------------------%

\begin{frame}{Cross Validation}

  To train a model that best predicts new data, we can use
  \emph{cross-validation} to evaluate the expected predictive performance on new
  data.
  \vc
  \begin{enumerate}
  \item Split the sample into two, disjoint sub-samples
    \begin{itemize}
    \item \emph{Training} data
    \item \emph{Testing} data
    \end{itemize}
    \vc
  \item Estimate a candidate model, $f(\mathbf{X})$, on the training data.
    \label{train}
    \vc
  \item Check the predictive performance of $\hat{f}(\mathbf{X})$ on the
    testing data. \label{test}
  \end{enumerate}
  \vb
  \pause
  We can use this idea to select the best model from a pool of candidate models,
  $\mathcal{F} = \left\{f_1(X), f_2(X), \ldots, f_J(X)\right\}$
  \vc
  \begin{enumerate}
  \item Repeat Steps \ref{train} and \ref{test} for all candidate models in
    $\mathcal{F}$.
    \vc
  \item Pick the $\hat{f}_j(\mathbf{X})$ that best predicts the testing data.
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Different Flavors of Cross-Validation}

  In practice, the split-sample cross-validation procedure describe above can be
  highly variable.
  \vc
  \begin{itemize}
  \item The solution is highly sensitive to the way the sample is split because
    each model is only training once.
  \end{itemize}
  \vb
  Split-sample cross-validation can also be wasteful.
  \vc
  \begin{itemize}
  \item We don't need to set aside an entire chunk of data for validation.
  \end{itemize}
  \vb
  In most cases, we will want to employ a slightly more complex flavor of
  cross-validation:
  \vc
  \begin{itemize}
  \item \emph{K-fold cross-validation}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{\emph{K}-Fold Cross-Validation}

  \begin{enumerate}
  \item Partition the data into $K$ disjoint subsets $C_k = C_1, C_2, \ldots,
    C_K$.
    \pause
    \vb
  \item Conduct $K$ training replications.
    \vb
    \begin{itemize}
    \item For each training replication, collapse $K - 1$ partitions into
      a set of training data, and use this training data to estimate the model.
      \vb
    \item Compute the test MSE for the $k$th partition, $MSE_k$, by using
      subset $C_k$ as the test data for the $k$th fitted model.
    \end{itemize}
    \pause
    \vb
  \item Compute the overall \kfold~error as:
  \end{enumerate}
  \begin{align*}
    CVE = \sum_{k = 1}^K \frac{N_k}{N}MSE_k,
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\emph{K}-Fold Cross-Validation}

<<eval = FALSE>>=
kFoldCV <- function(model, data, K) {
    ## Create a partition vector:
    part <- rep(1 : K, nrow(data) / K)

    ## Loop over K repetitions:
    mse <- c()
    for(k in 1 : K) {
        ## Partition data:
        train <- data[part != k, ]
        test  <- data[part == k, ]

        ## Fit model, generate predictions, and save MSE:
        fit    <- lm(model, data = train)
        pred   <- predict(fit, newdata = test)
        mse[k] <- MSE(y_pred = pred,
                      y_true = test[ , dvName(fit)])
    }
    ## Return the CVE:
    mean(mse)
}
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{$K$-Fold Cross-Validation}

<<eval = FALSE>>=
## Do K-Fold Cross-Validation with lm():
cv.lm <- function(data, models, K) {
    ## Create a partition vector:
    part <- rep(1 : K, nrow(data) / K)

    ## Permute the partition vector:
    part <- sample(part)

    ## Apply over candidate models:
    sapply(models, getCve, data = data, K = K, part = part)
}
@

\end{frame}

% ------------------------------------------------------------------------------%

\begin{frame}[fragile]{$K$-Fold Cross-Validation}

  So how does \kfold~perform in our $BP$ prediction task?

<<>>=
cvOut <- cv.lm(data   = diabetes,
               models = c("bp ~ age + bmi + ldl + hdl",
                          "bp ~ age + bmi"),
               K      = 10)
cvOut
@

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Applying \emph{K}-Fold CV to our Example}

  \begin{columns}
    \begin{column}{0.4\textwidth}

<<fig.show = "hide">>=
cvOutF <- CVlm(data = diabetes,
               form.lm = outF,
               m = 5,
               printit = FALSE,
               seed = 235711)

## Estimated CVE:
attr(cvOutF, "ms")
@

\end{column}
\begin{column}{0.6\textwidth}

<<echo = FALSE>>=
cvOutF <-
    CVlm(data = diabetes,
         form.lm = outF,
         m = 5,
         printit = FALSE,
         seed = 235711)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Applying \emph{K}-Fold CV to our Example}

  \begin{columns}
    \begin{column}{0.4\textwidth}

<<fig.show = "hide">>=
cvOutR <- CVlm(data = diabetes,
               form.lm = outR,
               m = 5,
               printit = FALSE,
               seed = 235711)

## Estimated CVE:
attr(cvOutR, "ms")
@

\end{column}
\begin{column}{0.6\textwidth}

<<echo = FALSE>>=
cvOutR <-
    CVlm(data = diabetes,
         form.lm = outR,
         m = 5,
         printit = FALSE,
         seed = 235711)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
