%%% Title:    DSS Stats & Methods: Lecture 7
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2020-09-25

\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.69,0.494,0}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.749,0.012,0.012}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.514,0.506,0.514}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0,0.341,0.682}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.004,0.004,0.506}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{Categorical Predictors}
\subtitle{Statistics \& Methodology Lecture 7}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}



%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}

  \begin{enumerate}
  \item Adding categorical predictors into MLR models
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Categorical Predictors}
  
  Most of the predictors we've considered thus far have been \emph{quantitative}.
  \vc
  \begin{itemize}
  \item Continuous variables that can take any real value in their range
    \vc
  \item Interval or Ratio scaling
    \vc
  \item If we use ordinal items as predictors, we assume interval scaling.
  \end{itemize}
  \vb
  We often want to include grouping factors as predictors.
  \vc
  \begin{itemize}
  \item These variables are \emph{qualitative}.
    \begin{itemize}
    \item Their values are simply labels.
    \item There is no ordering of the categories.
    \item Nominal scaling
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{How to Model Categorical Predictors}
  
  We need to be careful when we include categorical predictors into a regression
  model.
  \vc
  \begin{itemize}
  \item The variables need to be coded before entering the model.
  \end{itemize}
  \vb 
  Consider the following indicator of major: 
  $X_{maj} = \{1 = \textit{Law}, 2 = \textit{Economics}, 
  3 = \textit{Data Science}\}$ 
  \vc
  \begin{itemize}
    \item What would happen if we na\"ively used this variable to predict
      program satisfaction?
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{How to Model Categorical Predictors}
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mDat} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"../data/major_data.rds"}\hlstd{)}
\hlstd{mDat[}\hlkwd{seq}\hlstd{(}\hlnum{25}\hlstd{,} \hlnum{150}\hlstd{,} \hlnum{25}\hlstd{), ]}
\end{alltt}
\begin{verbatim}
##     sat majF majN
## 25  1.9  law    1
## 50  1.4  law    1
## 75  4.3 econ    2
## 100 4.1 econ    2
## 125 5.7   ds    3
## 150 5.1   ds    3
\end{verbatim}
\begin{alltt}
\hlstd{out} \hlkwb{<-} \hlkwd{lm}\hlstd{(sat} \hlopt{~} \hlstd{majN,} \hlkwc{data} \hlstd{= mDat)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{How to Model Categorical Predictors}
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept) -0.33200    0.12060  -2.753  0.00664
## majN         2.04500    0.05582  36.632  < 2e-16
## 
## Residual standard error: 0.5582 on 148 degrees of freedom
## Multiple R-squared:  0.9007,	Adjusted R-squared:    0.9 
## F-statistic:  1342 on 1 and 148 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Dummy Coding}
  
  The most common way to code categorical predictors is \emph{dummy coding}.
  \vb
  \begin{itemize}
  \item A $G$-level factor (i.e., one that represents $G$ groups) will be 
    transformed into a set of $G - 1$ dummy codes.
    \vb
  \item Each code is a variable on the dataset that equals 1 for observations
    corresponding to the code's group and equals 0, otherwise.
    \vb
  \item The group without a code is called the \emph{reference group}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Dummy Code} 
 Let's look at the simple example of coding biological sex: 
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlr}
  \toprule
 & sex & male \\ 
  \midrule
1 & female & 0 \\ 
  2 & male & 1 \\ 
  3 & male & 1 \\ 
  4 & female & 0 \\ 
  5 & male & 1 \\ 
  6 & female & 0 \\ 
  7 & female & 0 \\ 
  8 & male & 1 \\ 
  9 & female & 0 \\ 
  10 & female & 0 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Dummy Codes} 
 Now, a slightly more complex  example: 
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlrr}
  \toprule
 & drink & juice & tea \\ 
  \midrule
1 & juice & 1 & 0 \\ 
  2 & coffee & 0 & 0 \\ 
  3 & tea & 0 & 1 \\ 
  4 & tea & 0 & 1 \\ 
  5 & tea & 0 & 1 \\ 
  6 & tea & 0 & 1 \\ 
  7 & juice & 1 & 0 \\ 
  8 & tea & 0 & 1 \\ 
  9 & coffee & 0 & 0 \\ 
  10 & juice & 1 & 0 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Using Dummy Codes}
  
  To use the dummy codes, we simply include the $G - 1$ codes as $G - 1$
  predictor variables in our regression model.
  \begin{align*}
    Y &= \beta_0 + \beta_1 X_{male} + \varepsilon\\[6pt]
    Y &= \beta_0 + \beta_1 X_{juice} + \beta_2 X_{tea} + \varepsilon
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item The intercept corresponds to the mean of $Y$ in the reference group.
    \vb
  \item Each slope represents the difference between the mean of $Y$ in the 
    coded group and the mean of $Y$ in the reference group.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
  
  First, an example with a single, binary dummy-coded variable:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Read in some data:}
\hlstd{cDat} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"../data/cars_data.rds"}\hlstd{)}

\hlcom{## Fit and summarize the model:}
\hlstd{out1} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{mtOpt,} \hlkwc{data} \hlstd{= cDat)}
\end{alltt}
\end{kframe}
\end{knitrout}

\pagebreak

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out1,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   23.841      1.623  14.691   <2e-16
## mtOpt         -6.603      2.004  -3.295   0.0014
## 
## Residual standard error: 9.18 on 91 degrees of freedom
## Multiple R-squared:  0.1066,	Adjusted R-squared:  0.09679 
## F-statistic: 10.86 on 1 and 91 DF,  p-value: 0.001403
\end{verbatim}
\end{kframe}
\end{knitrout}

\pagebreak
  
Fit a more complex model:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{out2} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front} \hlopt{+} \hlstd{rear,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out2,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept) 17.63000    2.76119   6.385 7.33e-09
## front       -0.09418    2.96008  -0.032  0.97469
## rear        11.32000    3.51984   3.216  0.00181
## 
## Residual standard error: 8.732 on 90 degrees of freedom
## Multiple R-squared:  0.2006,	Adjusted R-squared:  0.1829 
## F-statistic: 11.29 on 2 and 90 DF,  p-value: 4.202e-05
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}
  
  Include two sets of dummy codes:
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{out3} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{mtOpt} \hlopt{+} \hlstd{front} \hlopt{+} \hlstd{rear,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out3,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  21.7187     2.9222   7.432 6.25e-11
## mtOpt        -5.8410     1.8223  -3.205  0.00187
## front        -0.2598     2.8189  -0.092  0.92677
## rear         10.5169     3.3608   3.129  0.00237
## 
## Residual standard error: 8.314 on 89 degrees of freedom
## Multiple R-squared:  0.2834,	Adjusted R-squared:  0.2592 
## F-statistic: 11.73 on 3 and 89 DF,  p-value: 1.51e-06
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Cell-Means Coding}
  
  If we include all $G$ dummy codes, we get a \emph{cell-means} coded model.
  \begin{itemize}
  \item Cell-means coding estimates the so-called \emph{normal means model}:
    \begin{align*}
      Y = \mu_g + \varepsilon
    \end{align*}
  \item We directly estimate each group-specific mean.
    \vc
  \item We cannot estimate an intercept when using cell-means coded predictors.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Cell-Means Code} 
 Let's look at the cell-means coding of biological sex: 
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlrr}
  \toprule
 & sex & female & male \\ 
  \midrule
1 & female & 1 & 0 \\ 
  2 & male & 0 & 1 \\ 
  3 & male & 0 & 1 \\ 
  4 & female & 1 & 0 \\ 
  5 & male & 0 & 1 \\ 
  6 & female & 1 & 0 \\ 
  7 & female & 1 & 0 \\ 
  8 & male & 0 & 1 \\ 
  9 & female & 1 & 0 \\ 
  10 & female & 1 & 0 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Cell-Means Codes} 
 Now, cell-means for the drinks example: 
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlrrr}
  \toprule
 & drink & coffee & juice & tea \\ 
  \midrule
1 & juice & 0 & 1 & 0 \\ 
  2 & coffee & 1 & 0 & 0 \\ 
  3 & tea & 0 & 0 & 1 \\ 
  4 & tea & 0 & 0 & 1 \\ 
  5 & tea & 0 & 0 & 1 \\ 
  6 & tea & 0 & 0 & 1 \\ 
  7 & juice & 0 & 1 & 0 \\ 
  8 & tea & 0 & 0 & 1 \\ 
  9 & coffee & 1 & 0 & 0 \\ 
  10 & juice & 0 & 1 & 0 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Using Cell-Means Codes}
  
  When using cell-means codes, we include all $G$ codes into our model, so we 
  must not estimate an intercept:
   \begin{align*}
    Y &= \beta_1 X_{female} + \beta_2 X_{male} + \varepsilon\\[6pt]
    Y &= \beta_1 X_{coffee} + \beta_2 X_{juice} + \beta_3 X_{tea} + \varepsilon
   \end{align*}
   \vx{-12}
   \begin{itemize}
   \item Each ``slope'' is an estimate of the group-specific mean of $Y$ in the 
     coded group.
     \vc
   \item The significance tests for the ``slopes'' are testing if the 
     group-specific means are different from zero.
   \end{itemize}
   
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
  
  First, an example with a two-level cell-means coded variable:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Read in some data:}
\hlstd{cDat} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"../data/cars_data.rds"}\hlstd{)}

\hlcom{## Fit and summarize the model:}
\hlstd{out4} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{atOnly} \hlopt{+} \hlstd{mtOpt} \hlopt{-} \hlnum{1}\hlstd{,} \hlkwc{data} \hlstd{= cDat)}

\hlcom{## HACK: Add a new class attribute to dispatch }
\hlcom{##       summary.cellMeans() in place of summary.lm():}
\hlkwd{class}\hlstd{(out4)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"cellMeans"}\hlstd{,} \hlkwd{class}\hlstd{(out4))}
\end{alltt}
\end{kframe}
\end{knitrout}

\pagebreak

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out4,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##        Estimate Std. Error t value Pr(>|t|)
## atOnly   23.841      1.623   14.69   <2e-16
## mtOpt    17.238      1.175   14.67   <2e-16
## 
## Residual standard error: 9.18 on 91 degrees of freedom
## Multiple R-squared:  0.1066,	Adjusted R-squared:  0.09679 
## F-statistic: 10.86 on 1 and 91 DF,  p-value: 0.001403
\end{verbatim}
\end{kframe}
\end{knitrout}

\pagebreak
  
Fit a model with a three-level factor:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{out5} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{four} \hlopt{+} \hlstd{front} \hlopt{+} \hlstd{rear} \hlopt{-} \hlnum{1}\hlstd{,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{class}\hlstd{(out5)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"cellMeans"}\hlstd{,} \hlkwd{class}\hlstd{(out5))}
\hlkwd{partSummary}\hlstd{(out5,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##       Estimate Std. Error t value Pr(>|t|)
## four    17.630      2.761   6.385 7.33e-09
## front   17.536      1.067  16.439  < 2e-16
## rear    28.950      2.183  13.262  < 2e-16
## 
## Residual standard error: 8.732 on 90 degrees of freedom
## Multiple R-squared:  0.2006,	Adjusted R-squared:  0.1829 
## F-statistic: 11.29 on 2 and 90 DF,  p-value: 4.202e-05
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Effects Coding}
  
  Another useful form of categorical variable coding is \emph{effects coding}.
  \vc
  \begin{itemize}
  \item Effects codes can be \emph{weighted} or \emph{unweighted}.
  \end{itemize}
  \vb
  \pause
  We'll first discuss \emph{unweighted} effects codes.
  \vc
  \begin{itemize}
  \item Unweighted effects codes are identical to dummy codes except that 
    ``reference group'' rows get values of -1 on all codes.
    \vc
  \item The intercept is interpreted as the unweighted mean of the 
    group-specific means of $Y$.
    \vc
  \item The slope associated with each code represents the difference between 
    the coded group's mean of $Y$ and the mean of the group-specific means of 
    $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Example Unweighted Effects Codes}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlr}
  \toprule
 & sex & male.ec \\ 
  \midrule
1 & female & -1 \\ 
  2 & male & 1 \\ 
  3 & male & 1 \\ 
  4 & female & -1 \\ 
  5 & male & 1 \\ 
  6 & female & -1 \\ 
  7 & female & -1 \\ 
  8 & male & 1 \\ 
  9 & female & -1 \\ 
  10 & female & -1 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{column}
    \begin{column}{0.5\textwidth}
      
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\begin{tabular}{rlrr}
  \toprule
 & drink & juice.ec & tea.ec \\ 
  \midrule
1 & juice & 1 & 0 \\ 
  2 & coffee & -1 & -1 \\ 
  3 & tea & 0 & 1 \\ 
  4 & tea & 0 & 1 \\ 
  5 & tea & 0 & 1 \\ 
  6 & tea & 0 & 1 \\ 
  7 & juice & 1 & 0 \\ 
  8 & tea & 0 & 1 \\ 
  9 & coffee & -1 & -1 \\ 
  10 & juice & 1 & 0 \\ 
   \bottomrule
\end{tabular}
\end{table}


\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Using Unweighted Effects Codes}
  
  We use the unweighted effects codes as we would use dummy codes.
  \vc
  \begin{itemize}
  \item We include the $G - 1$ effects codes as $G - 1$ predictor variables in 
    our regression model.
    \begin{align*}
      Y &= \beta_0 + \beta_1 X_{male.ec} + \varepsilon\\[6pt]
      Y &= \beta_0 + \beta_1 X_{juice.ec} + \beta_2 X_{tea.ec} + \varepsilon
    \end{align*}
  \item The intercept corresponds to the unweighted mean of the group-specific 
    means of $Y$.
    \vc
  \item Each slope represents the difference between the mean of $Y$ in the 
    coded group and the mean of the group-specific means of $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Model with single effects code:}
\hlstd{out6} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{mtOpt.ec,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out6,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   20.539      1.002  20.501   <2e-16
## mtOpt.ec      -3.301      1.002  -3.295   0.0014
## 
## Residual standard error: 9.18 on 91 degrees of freedom
## Multiple R-squared:  0.1066,	Adjusted R-squared:  0.09679 
## F-statistic: 10.86 on 1 and 91 DF,  p-value: 0.001403
\end{verbatim}
\end{kframe}
\end{knitrout}

\pagebreak

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Model with two effects codes (for a variable with G = 3):}
\hlstd{out7} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front.ec} \hlopt{+} \hlstd{rear.ec,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out7,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   21.372      1.226  17.433  < 2e-16
## front.ec      -3.836      1.372  -2.796  0.00632
## rear.ec        7.578      1.758   4.310 4.16e-05
## 
## Residual standard error: 8.732 on 90 degrees of freedom
## Multiple R-squared:  0.2006,	Adjusted R-squared:  0.1829 
## F-statistic: 11.29 on 2 and 90 DF,  p-value: 4.202e-05
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Why is $\hat{\beta}_0$ the Unweighted Mean of $Y$?}
  
  First, define the group-specific means:
  \begin{align*}
    \hat{\mu}_1 &= \hat{\beta}_0 + \hat{\beta}_1 (1) \text{\hspace{7pt}} + 
    \hat{\beta}_2 (0) \text{\hspace{6pt}} = \hat{\beta}_0 + \hat{\beta}_1\\
    \hat{\mu}_2 &= \hat{\beta}_0 + \hat{\beta}_1 (0) \text{\hspace{7pt}} + 
    \hat{\beta}_2 (1) \text{\hspace{6pt}} = \hat{\beta}_0 + \hat{\beta}_2\\
    \hat{\mu}_3 &= \hat{\beta}_0 + \hat{\beta}_1 (-1) + \hat{\beta}_2 (-1) = 
    \hat{\beta}_0 - \hat{\beta}_1 - \hat{\beta}_2
  \end{align*}

  \pause
  
  \begin{columns}[T]
    \begin{column}{0.01\textwidth}
    \end{column}
    
    \begin{column}{0.44\textwidth}
      Next, solve for $\hat{\beta}_1$ and $\hat{\beta}_2$:
      \begin{align*}
        \hat{\beta}_1 &= \hat{\mu}_1 - \hat{\beta}_0\\
        \hat{\beta}_2 &= \hat{\mu}_2 - \hat{\beta}_0\\
      \end{align*}
    \end{column}
  
    \pause
    
    \begin{column}{0.55\textwidth}
      Finally, substitute and solve for $\hat{\beta}_0$:
      \begin{align*}
        \hat{\mu}_3 &= \hat{\beta}_0 - (\hat{\mu}_1 - \hat{\beta}_0) - (\hat{\mu}_2 - \hat{\beta}_0)\\
        \hat{\mu}_3 &= 3\hat{\beta}_0 - \hat{\mu}_1 - \hat{\mu}_2\\
        \hat{\beta}_0 &= \frac{\hat{\mu}_1 + \hat{\mu}_2 + \hat{\mu}_3}{3}
      \end{align*}
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{Weighted Effects Coding}
  
  Weighted effects codes differ from the unweighted version only in how they 
  code the ``reference group'' rows. 
  \vb
  \begin{itemize}
  \item In weighted effects codes the ``reference group'' rows get negative 
    fractional values on all codes.
    \vc
    \begin{itemize}
    \item Let $g = 1, 2, \ldots, G$ index groups.
    \item Take the first group as the ``reference group.'' 
    \item Then, the $g$th code's reference group rows will take values of 
      $-N_g/N_1$.
    \end{itemize}
    \vb
  \item The intercept is interpreted as the weighted mean of the group-specific 
    outcome means.
    \vc
    \begin{itemize}
    \item The arithmetic mean of $Y$.
    \end{itemize}
    \vc
  \item Each slope represents the difference from that group's mean outcome and 
    the overall mean of $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Example Weighted Effects Codes}
  
  \begin{columns}
    \begin{column}{0.4\textwidth}
      
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{rll}
  \toprule
 & sex & male.wec \\ 
  \midrule
1 & female & $-N_{male}/N_{female}$ \\ 
  2 & male & 1 \\ 
  3 & male & 1 \\ 
  4 & female & $-N_{male}/N_{female}$ \\ 
  5 & male & 1 \\ 
  6 & female & $-N_{male}/N_{female}$ \\ 
  7 & female & $-N_{male}/N_{female}$ \\ 
  8 & male & 1 \\ 
  9 & female & $-N_{male}/N_{female}$ \\ 
  10 & female & $-N_{male}/N_{female}$ \\ 
   \bottomrule
\end{tabular}
}
\end{table}


\end{column}
    \begin{column}{0.6\textwidth}
      
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Sep 25 10:52:34 2020
\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{rlll}
  \toprule
 & drink & juice.wec & tea.wec \\ 
  \midrule
1 & juice & 1 & 0 \\ 
  2 & coffee & $-N_{juice}/N_{coffee}$ & $-N_{tea}/N_{coffee}$ \\ 
  3 & tea & 0 & 1 \\ 
  4 & tea & 0 & 1 \\ 
  5 & tea & 0 & 1 \\ 
  6 & tea & 0 & 1 \\ 
  7 & juice & 1 & 0 \\ 
  8 & tea & 0 & 1 \\ 
  9 & coffee & $-N_{juice}/N_{coffee}$ & $-N_{tea}/N_{coffee}$ \\ 
  10 & juice & 1 & 0 \\ 
   \bottomrule
\end{tabular}
}
\end{table}


\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Using Weighted Effects Codes}
  
  Weighted effects codes work the same way as all of our other codes.
  \vb
  \begin{itemize}
  \item As before, we include the $G - 1$ effects codes as $G - 1$ predictor 
    variables in our regression model.
    \begin{align*}
      Y &= \beta_0 + \beta_1 X_{male.wec} + \varepsilon\\[6pt]
      Y &= \beta_0 + \beta_1 X_{juice.wec} + \beta_2 X_{tea.wec} + \varepsilon
    \end{align*}
  \item The intercept corresponds to the weighted mean of the group-specific 
    means of $Y$ (i.e., the arithmetic average of $Y$).  
    \vb
  \item Each slope represents the difference between the coded group's mean of 
    $Y$ and the overall mean of $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Model with single effects code:}
\hlstd{out8} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{mtOpt.wec,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out8,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  19.5097     0.9519  20.495   <2e-16
## mtOpt.wec    -2.2720     0.6895  -3.295   0.0014
## 
## Residual standard error: 9.18 on 91 degrees of freedom
## Multiple R-squared:  0.1066,	Adjusted R-squared:  0.09679 
## F-statistic: 10.86 on 1 and 91 DF,  p-value: 0.001403
\end{verbatim}
\end{kframe}
\end{knitrout}

\pagebreak

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Model with two effects codes (for a variable with G = 3):}
\hlstd{out9} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front.wec} \hlopt{+} \hlstd{rear.wec,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out9,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  19.5097     0.9054  21.547  < 2e-16
## front.wec    -1.9739     0.5640  -3.500 0.000727
## rear.wec      9.4403     1.9863   4.753 7.57e-06
## 
## Residual standard error: 8.732 on 90 degrees of freedom
## Multiple R-squared:  0.2006,	Adjusted R-squared:  0.1829 
## F-statistic: 11.29 on 2 and 90 DF,  p-value: 4.202e-05
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[shrink = 5]{Why is $\hat{\beta}_0$ the Weighted Mean of $Y$?}
  
  \begin{columns}[T]
    \begin{column}{0.4\textwidth}
      
      Define the group-specific means:
      \begin{align*}
        \hat{\mu}_1 &= \hat{\beta}_0 + \hat{\beta}_1 (1) \text{\hspace{20pt}} + 
        \hat{\beta}_2 (0)\\[6pt]
        \hat{\mu}_2 &= \hat{\beta}_0 + \hat{\beta}_1 (0) \text{\hspace{20pt}} + 
        \hat{\beta}_2 (1)\\
        \hat{\mu}_3 &= \hat{\beta}_0 + \hat{\beta}_1 \left( \frac{-N_1}{N_3} \right) + \hat{\beta}_2 \left( \frac{-N_2}{N_3} \right)
      \end{align*}
      
      \pause
      
      Solve for $\hat{\beta}_1$ and $\hat{\beta}_2$:
      \begin{align*}
        \hat{\beta}_1 &= \hat{\mu}_1 - \hat{\beta}_0\\
        \hat{\beta}_2 &= \hat{\mu}_2 - \hat{\beta}_0\\
      \end{align*}
    \end{column}
    
    \pause
    
    \begin{column}{0.6\textwidth}
      Substitute and solve for $\hat{\beta}_0$:
      \begin{align*}
        \hat{\mu}_3 &= \hat{\beta}_0 + \left( \frac{-N_1}{N_3} \right) (\hat{\mu}_1 - \hat{\beta}_0) + \left( \frac{-N_2}{N_3} \right) (\hat{\mu}_2 - \hat{\beta}_0)\\[10pt]
        \hat{\mu}_3 &= \frac{N_3}{N_3}\hat{\beta}_0 - \frac{N_1}{N_3}\hat{\mu}_1 + \frac{N_1}{N_3}\hat{\beta}_0 - \frac{N_2}{N_3}\hat{\mu}_2 + \frac{N_2}{N_3}\hat{\beta}_0\\[10pt]
        \hat{\mu}_3 &= \frac{N_1 + N_2 + N_3}{N_3}\hat{\beta}_0 - \frac{N_1}{N_3}\hat{\mu}_1 - \frac{N_2}{N_3}\hat{\mu}_2\\[10pt]
        N_3\hat{\mu}_3 &= (N_1 + N_2 + N_3)\hat{\beta}_0 - N_1\hat{\mu}_1 - N_2\hat{\mu}_2\\[10pt]
        \hat{\beta}_0 &= \frac{N_1\hat{\mu}_1 + N_2\hat{\mu}_2 + N_3\hat{\mu}_3}{N_1 + N_2 + N_3}
      \end{align*}
    \end{column}
  \end{columns}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
  
  For variables with only two levels, we can test the overall factor's 
  significance by evaluating the significance of its single code.
  \begin{itemize}
  \item This won't work when we're using cell-means coding.
  \item Cell-means coding will always produce two or more codes.
  \end{itemize}
 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out1,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   23.841      1.623  14.691   <2e-16
## mtOpt         -6.603      2.004  -3.295   0.0014
## 
## Residual standard error: 9.18 on 91 degrees of freedom
## Multiple R-squared:  0.1066,	Adjusted R-squared:  0.09679 
## F-statistic: 10.86 on 1 and 91 DF,  p-value: 0.001403
\end{verbatim}
\end{kframe}
\end{knitrout}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
  
  For variables with more than two levels (or whenever using cell-means codes), 
  we need to simultaneously evaluate the significance of all of the variable's 
  codes.
 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out3,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  21.7187     2.9222   7.432 6.25e-11
## mtOpt        -5.8410     1.8223  -3.205  0.00187
## front        -0.2598     2.8189  -0.092  0.92677
## rear         10.5169     3.3608   3.129  0.00237
## 
## Residual standard error: 8.314 on 89 degrees of freedom
## Multiple R-squared:  0.2834,	Adjusted R-squared:  0.2592 
## F-statistic: 11.73 on 3 and 89 DF,  p-value: 1.51e-06
\end{verbatim}
\end{kframe}
\end{knitrout}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(out3)}\hlopt{$}\hlstd{r.squared} \hlopt{-} \hlkwd{summary}\hlstd{(out1)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.1767569
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(out1, out3)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: price ~ mtOpt
## Model 2: price ~ mtOpt + front + rear
##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
## 1     91 7668.9                                  
## 2     89 6151.6  2    1517.3 10.976 5.488e-05 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
  
  What about models where a single nominal factor is the only predictor?

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{partSummary}\hlstd{(out2,} \hlopt{-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept) 17.63000    2.76119   6.385 7.33e-09
## front       -0.09418    2.96008  -0.032  0.97469
## rear        11.32000    3.51984   3.216  0.00181
## 
## Residual standard error: 8.732 on 90 degrees of freedom
## Multiple R-squared:  0.2006,	Adjusted R-squared:  0.1829 
## F-statistic: 11.29 on 2 and 90 DF,  p-value: 4.202e-05
\end{verbatim}
\end{kframe}
\end{knitrout}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
  
  We can compare back to an ``intercept-only'' model.
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{out0} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlnum{1}\hlstd{,} \hlkwc{data} \hlstd{= cDat)}
\hlkwd{partSummary}\hlstd{(out0,} \hlopt{-}\hlnum{1}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Residuals:
##    Min     1Q Median     3Q    Max 
## -12.11  -7.31  -1.81   3.79  42.39 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)   19.510      1.002   19.48   <2e-16
## 
## Residual standard error: 9.659 on 92 degrees of freedom
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{r2Diff} \hlkwb{<-} \hlkwd{summary}\hlstd{(out2)}\hlopt{$}\hlstd{r.squared} \hlopt{-} \hlkwd{summary}\hlstd{(out0)}\hlopt{$}\hlstd{r.squared}
\hlstd{r2Diff}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(out0, out2)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: price ~ 1
## Model 2: price ~ front + rear
##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
## 1     92 8584.0                                  
## 2     90 6861.7  2    1722.3 11.295 4.202e-05 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Significance Testing for Categorical Variables}
 
  We don't actually need to do the explicit model comparison, though.
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{r2Diff}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(out2)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(out0, out2)[}\hlnum{2}\hlstd{,} \hlstr{"F"}\hlstd{]}
\end{alltt}
\begin{verbatim}
## [1] 11.29494
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(out2)}\hlopt{$}\hlstd{fstatistic[}\hlnum{1}\hlstd{]}
\end{alltt}
\begin{verbatim}
##    value 
## 11.29494
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Compare Codings}
  
  Let's dig into some numerical properties of the three coding schemes.
  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Fit models using all four codings:}
\hlstd{dcOut}  \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front} \hlopt{+} \hlstd{rear,}            \hlkwc{data} \hlstd{= cDat)}
\hlstd{cmOut}  \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{four} \hlopt{+} \hlstd{front} \hlopt{+} \hlstd{rear} \hlopt{-} \hlnum{1}\hlstd{,} \hlkwc{data} \hlstd{= cDat)}
\hlstd{ecOut}  \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front.ec} \hlopt{+} \hlstd{rear.ec,}      \hlkwc{data} \hlstd{= cDat)}
\hlstd{wecOut} \hlkwb{<-} \hlkwd{lm}\hlstd{(price} \hlopt{~} \hlstd{front.wec} \hlopt{+} \hlstd{rear.wec,}    \hlkwc{data} \hlstd{= cDat)}

\hlcom{## Compute group-specific means of 'price':}
\hlstd{grpMeans} \hlkwb{<-} \hlkwd{tapply}\hlstd{(cDat}\hlopt{$}\hlstd{price, cDat}\hlopt{$}\hlstd{dr, mean)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{}
Compare the parameter estimates to their theoretical equivalents:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{coef}\hlstd{(dcOut)[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{grpMeans[}\hlstr{"4WD"}\hlstd{]}
\end{alltt}
\begin{verbatim}
##   (Intercept) 
## -1.421085e-14
\end{verbatim}
\begin{alltt}
\hlkwd{coef}\hlstd{(cmOut)} \hlopt{-} \hlstd{grpMeans}
\end{alltt}
\begin{verbatim}
##           4WD         Front          Rear 
##  0.000000e+00  7.105427e-15 -3.552714e-15
\end{verbatim}
\begin{alltt}
\hlkwd{coef}\hlstd{(ecOut)[}\hlnum{1}\hlstd{]} \hlopt{-} \hlkwd{mean}\hlstd{(grpMeans)}
\end{alltt}
\begin{verbatim}
##   (Intercept) 
## -3.552714e-15
\end{verbatim}
\begin{alltt}
\hlkwd{coef}\hlstd{(wecOut)[}\hlnum{1}\hlstd{]} \hlopt{-} \hlkwd{mean}\hlstd{(cDat}\hlopt{$}\hlstd{price)}
\end{alltt}
\begin{verbatim}
##   (Intercept) 
## -1.065814e-14
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{}
  Compare the $R^2$ values from each coding scheme:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(dcOut)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{summary.cellMeans}\hlstd{(cmOut)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(ecOut)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(wecOut)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.2006386
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{}
Compare the F-statistics:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(dcOut)}\hlopt{$}\hlstd{fstatistic}
\end{alltt}
\begin{verbatim}
##    value    numdf    dendf 
## 11.29494  2.00000 90.00000
\end{verbatim}
\begin{alltt}
\hlkwd{summary.cellMeans}\hlstd{(cmOut)}\hlopt{$}\hlstd{fstatistic}
\end{alltt}
\begin{verbatim}
##    value    numdf    dendf 
## 11.29494  2.00000 90.00000
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(ecOut)}\hlopt{$}\hlstd{fstatistic}
\end{alltt}
\begin{verbatim}
##    value    numdf    dendf 
## 11.29494  2.00000 90.00000
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(wecOut)}\hlopt{$}\hlstd{fstatistic}
\end{alltt}
\begin{verbatim}
##    value    numdf    dendf 
## 11.29494  2.00000 90.00000
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{}
Compare the residual standard errors:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(dcOut)}\hlopt{$}\hlstd{sigma}
\end{alltt}
\begin{verbatim}
## [1] 8.731638
\end{verbatim}
\begin{alltt}
\hlkwd{summary.cellMeans}\hlstd{(cmOut)}\hlopt{$}\hlstd{sigma}
\end{alltt}
\begin{verbatim}
## [1] 8.731638
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(ecOut)}\hlopt{$}\hlstd{sigma}
\end{alltt}
\begin{verbatim}
## [1] 8.731638
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(wecOut)}\hlopt{$}\hlstd{sigma}
\end{alltt}
\begin{verbatim}
## [1] 8.731638
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Choosing a Coding Scheme}
  
  Any valid coding scheme will represent the information in the categorical 
  variable equally well.
  \vc
  \begin{itemize}
  \item All valid coding schemes produce equivalent models. 
  \end{itemize}
  \vb
  We choose a particular coding scheme based on the interpretations that we 
  want.
  \vc
  \begin{itemize}
  \item Dummy coding is useful with a meaningful reference group.
    \begin{itemize}
    \item Control group in an experiment
    \item An ``industry standard'' or benchmark implementation of some feature
    \end{itemize}
    \vc
  \item Dummy coding is also preferred if we don't care about interpretation.
    \begin{itemize}
    \item Dummy codes are the simplest to construct.
    \end{itemize}
    
    \pagebreak
    
  \item Cell-means coding is useful when you want to directly test for non-zero 
    means within each group.
    \begin{itemize}
    \item The interpretation of cell-means effects is probably the most 
      intuitive of any coding scheme, as well.
    \end{itemize}
    \vc
  \item Weighted effects codes are good when you believe your sample is 
    representative of the population.
    \begin{itemize}
    \item Larger groups should be weighted more heavily in the model.
      \vc
    \item Parameter estimates will correctly generalize to the population.
    \end{itemize}
    
    \pagebreak
    
  \item Unweighted effects codes are good when the group sizes in your sample do
    not generalize to the population.
    \begin{itemize}
    \item Convenience samples, for example, are usually not representative.
      \vc
    \item When your sample is not representative, larger groups should not be 
      weighted more heavily.
      \vc
    \item Unweighted effects codes are ``agnostic'' to differing group sizes.
      \begin{itemize}
      \item We need to be careful with very small groups.
      \end{itemize}
    \end{itemize}
    \vc
  \item Weighted effects codes with known weights are another option.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Conclusion}
  
  When we use categorical predictors, they must be coded before entering the 
  model.
  \vc
  \begin{itemize}
  \item We discussed four of the most popular coding schemes:
    \begin{enumerate}
    \item Dummy coding
    \item Cell-means coding
    \item Unweighted effects coding
    \item Weighted effects coding
    \end{enumerate}
    \vc
  \item Apart from cell-means, these coding schemes differ primarily in how the 
    ``reference'' group is defined.
  \end{itemize}
  \vc
  All valid coding schemes produce equivalent models.
  \begin{itemize} 
    \item We choose a particular scheme for interpretational convenience.
  \end{itemize}
  
\end{frame}

\comment{
\begin{frame}[allowframebreaks]{References}
  
  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtexStuff/statMethRefs.bib}
  
\end{frame}
}

%------------------------------------------------------------------------------%

\end{document}

